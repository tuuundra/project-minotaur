Thanks for the detailed clarifications. I’ll now begin a deep analysis on optimal aggregation methods for each of your intra-bar features. This will cover theoretical justifications, practical trade-offs, and references to relevant financial literature or industry practice, all within the constraint of working only from pre-aggregated short dollar bars.

I’ll get back to you soon with a structured, feature-by-feature aggregation strategy including pros, cons, and recommendations for potential improvements to your feature pipeline.


# Optimal Aggregation Techniques for Intra-Bar Features (BTC/USDT HFT)

Below we detail each feature and the recommended method to aggregate short dollar-bar features into coarser (medium/long) dollar bars. We provide justification, potential pitfalls, and notes on when more complex methods or raw data might be needed. The goal is to preserve as much information as possible when *only* short-bar features (and bar volumes) are available.

## 1. Trade Imbalance (Net Signed Volume)

* **Recommended Aggregation:** **Sum of net signed volumes** across the constituent short bars (i.e. sum of buy volume minus sell volume for all sub-bars comprising the larger bar). This effectively produces the net order flow imbalance for the combined period.
* **Justification:** Order flow imbalances are **additive by nature** – if each short bar has a net volume imbalance, their sum is the total imbalance over the longer interval. Summing **preserves the full cumulative buying vs. selling pressure** in the aggregated bar. This is statistically sound because net signed volume simply accumulates over disjoint intervals. In contrast, taking an arithmetic mean would dilute the imbalance (e.g. dividing by the number of sub-bars), misrepresenting the true pressure. A weighted mean is unnecessary given all short bars are equal dollar volume; summing inherently accounts for volume weighting (and if short bars were unequal, weighting by volume reduces to the same as summing in computing a total).
* **Considerations:** If a normalized metric is desired (e.g. an **imbalance ratio** relative to total volume), it should be computed *after* aggregation – for example, **aggregate\_buy\_volume − aggregate\_sell\_volume, divided by aggregate\_volume** (this will equal the volume-weighted average imbalance fraction). Summation itself has **no major pitfalls** here: it fully preserves information from the short bars. Only minimal information is lost (such as the timing of imbalance within the interval, which isn’t captured without raw ticks). In practice, this approach aligns with order flow toxicity measures like VPIN, where imbalances are summed over fixed-volume buckets and normalized by total volume – confirming that summing net order imbalance is the correct, information-preserving choice.

## 2. Intra-Bar Tick Price Volatility (Std. Dev. of Tick Prices)

* **Recommended Aggregation:** **Volume-weighted pooling of variance**, then take the square root to get standard deviation for the combined bar. In practice, treat each short bar’s variance (std²) and “sample size” (number of ticks, proxied by volume) and compute a pooled variance: for example, $\sigma^2_{\text{combined}} \approx \frac{\sum_i n_i \sigma^2_i}{\sum_i n_i}$ (plus terms to adjust for mean price differences if available). Volume of each bar can serve as a weight $n_i$ (assuming tick count ∝ volume).
* **Justification:** The **statistically rigorous** way to combine volatility from sub-samples is to compute the **pooled variance** accounting for each sub-bar’s variance and mean. The formula for merging two sets includes a correction for their mean difference. If we had each bar’s tick-count and mean price, we could exactly compute the overall variance (summing within-bar variance contributions and the variance due to mean shifts). In absence of full details, weighting by volume approximates giving more influence to bars with more ticks. This preserves the scale of price variability better than a simple average of standard deviations (or worse, taking an average of std devs which would underweight high-tick bars). Essentially, we assume each short bar provides a variance estimate, and we combine them in proportion to observations. This approach is analogous to **realized volatility** accumulation: if tick-by-tick data were available, the total variance over a period is the sum of variances of sub-intervals (plus covariance terms which are zero for non-overlapping periods).
* **Considerations:** **Pitfall:** simply averaging the standard deviations can be misleading – it doesn’t account for how many price observations contributed to each value. Our volume-weighted method mitigates that, but one **caveat** is that it ignores the potential **mean price drift between bars**. If the price level changed significantly from one short bar to the next, the true combined volatility would be higher than the within-bar variances suggest (because the overall price series spans a wider range). The pooled variance formula theoretically should include a term for the difference in means. If some proxy for mean price per bar is available (e.g. mid-bar price or open/close), one could incorporate it to adjust the variance (adding $\frac{n_i n_j}{N}(\mu_i - \mu_j)^2$ terms for each adjacent pair). Without such data, our method may **underestimate** volatility in a trending market. **Raw tick data** would be needed to perfectly recompute standard deviation across the combined bar – our weighted approach is an approximation. Nonetheless, it is **practical and usually accurate** for feature engineering, especially if short bars are small (so price doesn’t drift drastically within the aggregated window).

## 3. Taker Buy/Sell Volume Ratio

* **Recommended Aggregation:** **Recompute the ratio from aggregated volumes.** That is, derive the total taker-buy volume and total taker-sell volume over the combined interval and then take **combined\_ratio = (Σ taker\_buy) / (Σ taker\_sell)**. This can be done since for each short bar we have the ratio $r_i = B_i/S_i$ and total volume $V_i = B_i + S_i$. From these, one can solve $B_i = \frac{r_i}{1+r_i} V_i$ and $S_i = \frac{1}{1+r_i} V_i$. Summing all $B_i$ gives the total aggressive buy volume and summing all $S_i$ gives total sell volume, then take the ratio.
* **Justification:** The taker buy/sell ratio by definition **“measures the volume of market buy orders relative to market sell orders”**. To get this metric for a longer period, the only **correct and information-preserving** way is to aggregate the underlying buy and sell volumes. This effectively amounts to a **volume-weighted average** of the short-bar ratios, but doing it via the summed components guarantees exactness. If one instead took a simple average of the ratio values, it would be **biased** by the number of bars and not their volume – for example, a tiny-volume bar with an extreme ratio would skew the average despite contributing little volume. Even a naïve weighted average of ratios (weighted by bar volume) is algebraically equivalent to computing the overall ratio from summed volumes, so one might as well do the sum directly. This ensures the combined feature accurately reflects the true proportion of buy vs. sell volume in the period.
* **Considerations:** In implementation, care must be taken with edge cases: if the aggregated sell volume ΣS is zero (e.g. no taker sells at all across those bars, meaning pure buys), the ratio is theoretically infinite. In practice one might cap the ratio or set it to a very high value in such cases. However, this situation is extreme; typically combining bars yields a well-defined ratio as long as there’s at least some selling in the window. The **short-bar data is sufficient to compute this exactly**, so no information is lost in aggregation (aside from rounding errors). No more complex method is needed under current constraints – we already have total volumes and ratio, which is enough. Using raw tick data here wouldn’t improve the accuracy of the aggregated ratio (it would yield the same result since we’re effectively reconstructing the necessary sums). In summary, **summing volumes** for buys and sells is the **theoretically sound** approach, preserving the meaning of the ratio over longer intervals.

## 4. Tick Price Skewness

* **Recommended Aggregation:** **Volume-weighted average of skewness** (as an approximation). In absence of raw tick data, assign each short bar a weight proportional to its number of price observations (use total volume as a proxy) and compute a weighted mean of the skewness values. This gives a single skewness figure for the combined bar that leans more on bars that had more data/volume.
* **Justification:** Skewness is a **third-moment** statistic (measuring asymmetry), which does **not add linearly** like volumes do. The exact combined skewness would be derived from the **aggregate distribution of tick prices**. The correct approach mathematically is to combine the **central moments**: one would compute the overall mean price, overall third central moment, and then normalize by the cube of overall std. Achieving this requires having at least each sub-bar’s mean and variance in addition to skew to apply the formula. Since we only have the skew values (and total volumes), a weighted average is a pragmatic choice: it assumes each bar’s skewness estimate contributes in proportion to the bar’s size. This preserves information better than a plain average (which would over-emphasize small bars or treat all bars equally despite differing data sizes). It’s analogous to merging two distributions by giving more weight to the one with more observations.
* **Limitations & Pitfalls:** **Skewness does not aggregate cleanly**, so this method is an **approximation**. If the tick price distributions in the short bars have very different means or spreads, the true combined skewness could be quite different from any average. For instance, if one short bar’s prices are skewed left and the next bar’s are skewed right, the combined distribution might actually be more symmetric – our method might average the two skewness values and imply moderate skew when in reality the overall skewness could be near zero. Moreover, as we aggregate over longer periods, distributions often become more symmetric (Central Limit Theorem effect), so high-frequency skewness tends to diminish at lower frequency. A weighted average of sub-bar skewness might **overstate** the skew of the combined period because it doesn’t fully account for this normalization effect. **In practice**, if we had **raw tick data or at least the ability to compute aggregated moments**, we would do so: calculate the combined mean and use $\text{skew} = \frac{E[(P-\mu)^3]}{\sigma^3}$. Indeed, formulas exist to merge third moments from partitions of data. Without such data, one must accept some information loss. The weighted approach is usually acceptable as an **approximate indicator** of asymmetry for the larger bar. It is simple to implement and at least ensures that a bar comprising many short bars (with their own skews) doesn’t end up dominated by the skewness of a tiny segment.
* **When Raw Data Needed:** If skewness is a critical feature, the **best accuracy** comes from recalculating it with tick-by-tick prices in the merged bar. That would capture any cross-bar price movement effects (e.g. if the price trended, creating a skew in the combined distribution even if each sub-bar was symmetric, or vice versa). Raw data would also allow computing the third central moment exactly (summing $\langle X^3\rangle$ of each bar and adjusting for means). Given our constraints, raw data isn’t available, so the recommendation is to proceed with the weighted-average skewness as a **practical compromise**, while being aware of its potential divergence from the true combined skew.

## 5. Tick Price Kurtosis

* **Recommended Aggregation:** **Volume-weighted average of kurtosis** (approximate). Similar to skewness, assign weights based on each bar’s volume (or tick count) and compute a weighted mean of the kurtosis values from the short bars to represent the longer bar’s kurtosis. This treats larger bars as more influential in determining tail heaviness.
* **Justification:** Kurtosis (especially if excess kurtosis) measures the **tailedness** or outlier-proneness of the price distribution (fourth moment). Accurately computing combined kurtosis would entail merging the fourth central moments from each short bar – a complex calculation requiring means and lower moments from each bar. Without raw data, a weighted average is a **reasonable heuristic**: it ensures a bar with many ticks (hence a more reliable kurtosis estimate) contributes proportionally more to the combined value than a bar with few ticks. This is analogous to pooling variance or skewness in a weighted sense – while not exact, it attempts to account for sample size. It’s certainly better than a naive average that would give a tiny bar’s kurtosis equal importance as a large bar’s.
* **Limitations & Pitfalls:** **Kurtosis is highly sensitive** to extreme values, and combining distributions can dramatically change it. A big issue is that **aggregating tends to “normalize” the distribution** – extremely high kurtosis seen in ultra-short intervals often decreases when you look at a longer interval (as extreme moves get averaged with calmer periods). According to the Central Limit Theorem, as you aggregate more independent price changes, the distribution moves closer to normal (kurtosis 3). So if each short bar has, say, heavy tails (high kurtosis), the combined series might have a lower kurtosis than the weighted average would indicate. Our method may **overestimate tail risk** for the aggregated bar. Conversely, if one short bar had a one-off outlier price jump (huge kurtosis) but others were mild, a simple weighted average might underplay the chance that the combined period had that outlier – though in combined distribution that single outlier is less pronounced overall. In short, the weighted approach **cannot capture non-linear interactions** between segments (e.g. price mean shifts, overlaps of outliers). **No purely linear aggregator will be perfect for kurtosis.** The only way to get it precisely is to **recompute from raw tick prices**, or at least to store and combine the 4th moment and mean of each bar. Without those, we accept an approximation.
* **Practical Guidance:** Use the weighted average kurtosis as an **approximate feature** in the pipeline, but treat it with some caution. It will generally indicate if the combined bar’s price series had heavier tails versus normal, but small errors are likely. If the model is highly sensitive to kurtosis, consider engineering a proxy differently – for example, computing *realized kurtosis* using intrabar returns if possible (which would require raw data). Under current constraints, implementing the volume-weighted kurtosis is straightforward and keeps the feature in a comparable scale. Just be aware that **extreme intrabar events might get “washed out”** in the aggregation. If needed, one could complement this by flagging if any sub-bar had an extreme event (kurtosis or return outlier) as an additional feature. In summary, the weighted average is the **most practical method** given limited data, but additional data (tick-by-tick or at least aggregated moments) would be required to get a truly information-perfect kurtosis for the longer bar.

## 6. Number of Price Changes (Count of Unique Price Levels)

* **Recommended Aggregation:** *No exact aggregate is possible without raw price data.* Ideally, one would **compute the union of unique price levels** from all the short bars in the interval to find the true count for the combined bar. In practice (with only stored counts per bar), you may choose an approximation: for example, use the **sum of unique counts as an upper bound** and then adjust downwards with an estimate of overlap. Another rough approach is to use price range information – if you know the min and max price over the entire combined period, you could assume that prices in between were touched (if tick size is small) and estimate the count from that range. However, **these approaches are heuristic**. If precision is important, consider recomputing this feature from higher-resolution data.
* **Justification:** The number of unique price levels in a longer period is **not a simple sum** of the counts from sub-periods because many price levels repeat across bars. In set terms, if $P_1, P_2, \dots, P_n$ are the sets of prices in each short bar, we need |$P_1 \cup P_2 \cup \dots \cup P_n$|. By the inclusion-exclusion principle, for two bars $A$ and $B$:
  $n(A\cup B) = n(A) + n(B) - n(A\cap B)$
  . The term $n(A\cap B)$ (overlap) is unknown to us without tick data. For multiple bars it gets more complex (overlaps between many sets). Simply summing $n(A)+n(B)+\dots$ **over-counts** because it counts the same price level multiple times if it appears in multiple bars. On the other hand, **averaging** or other naive methods wouldn’t make sense here (the union count should logically be at least as large as the largest single-bar count, typically more). Therefore, without additional information, there is **no statistically sound one-shot formula** using just the stored counts. The only *theoretically correct* method is to identify unique prices globally – which requires the price data.
* **Considerations for Approximation:** If one still wants an approximate feature, you must **assume something about overlap**. For example, if the short bars are consecutive in time (which they are), and if the price doesn’t trend too far, there will be substantial overlap in price levels between adjacent bars. One could guess an overlap factor: e.g., assume adjacent bars share, say, 50% of their price levels in common if the price ranges overlap significantly. Then you might do: $\text{est\_unique} \approx n_1 + n_2 + \dots + n_n - \text{overlap\_adjustment}$. The overlap adjustment could be approximated using the proportion of price-range overlap between bars (if high/low of each bar are known). For instance, if bar2’s price range lies entirely inside bar1’s range, then nearly all of bar2’s “unique” prices were already seen, so the union ≈ max(n1, n2) rather than sum. If bar2 extends to new highs or lows beyond bar1, then it will contribute additional unique prices roughly equal to those new extensions divided by tick size. These are rough heuristics – **error is likely**. In implementation, a conservative route might be to **use the maximum** of the short-bar counts as a baseline (since the union can’t be less than the largest individual count) and add some fraction of the other bars’ counts. However, because this is complex and potentially unreliable, **practical guidance** is: if this feature is important, retrieve or store the necessary price information to compute it properly. If that’s not feasible, you might consider dropping or redefining this feature at the aggregated level because an inaccurate value could be misleading to the model.
* **Raw Data Requirement:** To truly get the number of unique price levels in a combined bar, **raw tick data or at least the sequence of trade prices is required**. With the tick prices, one would simply take the union of all prices in the interval and count them directly (this is straightforward set logic, and ensures no double counting). Given we only have aggregated features, **we cannot do better than an educated guess**. Thus, this is one case where additional data (or at least storing each bar’s price list or price range) would markedly improve accuracy. In summary, without raw data the **most “information-preserving” choice is not to mislead by summing** (which *overestimates* unique price count), but rather to acknowledge the limitation or use a rough union estimate if absolutely needed.

## 7. Number of Directional Changes (Price Trend Reversals)

* **Recommended Aggregation:** **Sum the directional changes of sub-bars**, *plus adjustments* for cross-bar transitions. Start by summing all recorded directional changes from the short bars. Then consider each boundary between bars: if the price trend at the end of Bar *i* and the beginning of Bar *i+1* are opposite, then the combined series has one additional directional change at that point which was not counted in either bar’s individual feature. In practical terms, one can infer this by looking at the **price at the end of one bar and the start of the next** (or the net direction of each bar as a proxy for the last tick direction). If they suggest a reversal, add 1 to the sum for that boundary. If they indicate continuity, add 0. This yields an estimated total count of direction changes for the merged bar.
* **Justification:** The count of directional changes (uptick to downtick or vice versa) in a combined period should account for **intra-bar changes (already counted in each short bar)** **and** **inter-bar changes at the junctions**. By summing the sub-bar counts, we include all the zig-zags that happened *within* each short bar. However, the end of one bar and the beginning of the next is a break in our data – a change at that boundary would not be recorded in either bar’s internal count. For example, suppose Bar A’s last price move was upward; Bar B (the next bar) starts and the first price movement in B is a downward move – effectively, there was a reversal in price direction as we go from A to B. In the continuous price series, that is a legitimate “direction change,” even though Bar A’s feature ends at its last internal change and Bar B’s feature starts fresh. Our aggregation method explicitly checks for this scenario and increments the count when it occurs. If instead the price was rising at end of A and continued to rise at start of B, then no new change happened at that transition (the trend continued uninterrupted), so we wouldn’t add an extra count. This logic ensures we **preserve the information** about trend reversals that span bar boundaries.
* **Considerations:** Without tick-by-tick data, determining the exact trend at the bar boundary is imperfect. We might use **bar-level proxies**: one simple proxy is the **net price change of the bar** (close minus open). If Bar i had a positive net change (implying overall upward movement) and the next bar has a negative net change (overall downward movement), it’s a strong hint that a directional reversal took place at or near the boundary. In that case, we add 1. If both bars have net price changes of the same sign, one might assume the general direction continued (no boundary change). This heuristic will catch many genuine reversals but can miss or misidentify some cases. For instance, it’s possible Bar i was up overall but ended with a downward swing – the next bar might continue that downward swing, meaning the reversal actually happened inside Bar i, not at the boundary, and our heuristic might falsely add a change at the boundary. Conversely, if bars are very short, net change might be zero or inconclusive while an actual flip-flop happened exactly at the break. **Despite these caveats, using net bar direction is a practical estimator** of boundary behavior. It leverages data we likely have (open and close of each bar) without needing full tick resolution. Another consideration is that if the data is **very high frequency**, there could be multiple rapid direction changes within what appears as one bar’s net trend – but those are already counted in the sub-bar feature. We only focus on the transition. Overall, this adjusted-sum method tends to **over-count less** than simply summing would (summing actually *under*-counts changes because it misses boundary flips), and it **preserves more information** about the price path.
* **When Raw Data is Necessary:** To be 100% accurate, one would need the **sequence of tick price changes across the entire combined interval**. Then counting directional changes is straightforward and exact. With raw data, you wouldn’t bother with this aggregated approach at all – you’d just count every time the price ticks from up to down or down to up. Since we cannot do that here, the described method is a **reasonable, implementable compromise**. It should work well if large trend reversals are the primary concern (those will usually be captured by the net-change sign flip logic). Minor inaccuracies might remain, but for most practical purposes in a feature engineering pipeline, this approach yields a good **representative count of direction changes** for coarser bars. If higher precision is required (for example, in research or if this feature critically affects the model), then incorporating a small window of tick data around bar boundaries or storing the last tick direction of each bar could be considered. Otherwise, the heuristic above provides a sound balance between **theoretical consistency and available information** for aggregation.

**References:**

* Lopez de Prado et al. (2012) – introduction of VPIN (Volume-Synchronized Probability of Informed Trading) – demonstrates aggregating order imbalance across volume bars and normalizing by total volume. This supports summing **trade imbalance** features rather than averaging.
* Formulae for combining statistical moments – shows how **variance, skewness, and kurtosis** for merged data can be derived from sub-samples’ moments, highlighting the need for information like means and higher moments to exactly aggregate **volatility, skewness,** and **kurtosis**.
* General set theory principle for **unique counts** – union of sets formula underpins why summing **unique price levels** from sub-bars double-counts overlapping prices, necessitating knowledge of intersections (unavailable without tick data).
* Discussion on distribution aggregation – higher-frequency data exhibit more extreme **skew/kurtosis**, which tend to diminish when aggregating (Central Limit Theorem effect). This contextualizes the caution in aggregating skewness and kurtosis from short bars.
* Definition of **Taker Buy/Sell ratio** – a ratio based on buy vs. sell volumes. This justifies aggregating by summing volumes for buys and sells to recompute the ratio for the combined interval (maintaining its meaning as a volume-based metric).
