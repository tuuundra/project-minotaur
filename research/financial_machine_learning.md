hello everyone welcome to the machine learning algorithm for financial markets so today we have Dr Edo VOR and he is a quantitative strategist and he interest in reinforcement learning and all the quantitative strategies and he is going to walk us through machine learning algorithm for the financial Market which with us and all his expertise in the real life scenario and all the details and after this session uh there will be a Q&A session where you can write down your questions and you can ask Dr Edo vtor directly and walk us through that so Dr VOR over to you okay thank you very much I'll share my screen yes Eduardo if you can maybe yes uh explain who you are what you do in your life what's your job maybe before starting I think it could be very interesting for uh a lot of people here sure sure of course so first of all uh thanks benam and the crunch out team for for this invitation I'm happy to share my experience with this community and it's great to see so many people here so I'm in a systematic trading team where I focus on creating intraday trading strategies with the use of machine learning and today's presentation has has the objective of giving you a bit an overview of what I'm doing and of uh uh how machine learning algorithms are used at least at least my experience in this field so let's uh let's begin and let's begin with an introduction on the use of algorithms in financial markets and the the use of algorithms in the markets is increasing there's actually estimates that the global algorithmic trading Market reached a value of more than 15 billion in 2023 where and it's expected to grow each year at around 10% but what are these algorithms used for so the majority are used for optimal execution which essentially means that you want to minimize your transaction cost and Market impact when trading on the financial markets then you have Market making algorithms which are quoting continuously on the bid and on the ask a number of assets uh algorithms in this this field are fundamental because now with the electronic markets things move extremely quickly we're talking about nanocs and so it's impossible in in these extremely liquid and fast- based markets for humans to to keep up with uh with algorithms then we have hedging algorithms which are used still in the case of Market making so if the market maker accumulates an inventory it may want to reduce its risk this is called hedging and it may be useful to have algorithms helping out with that and then we have trading and portfolio optimization algorithms which will be the focus of of today's presentation and which are Al algorithms that try and uh generate a profit so why uh why are algorithms becoming so popular and being used so much well we've uh we've talked about Market making they enable essentially the response the reduction of response time so if a client sends a request for quote it expects to have a response time an immediate response essentially and so algorithms can definitely help with that they help to reduce operational errors and Advantage is that they enable the analysis of data in in real time today we will be talking about the limit order book it's it's quite a large amount of data that gets updated at a high frequency algorithms enable this type of analysis what are the challenges when using algorithms well creating General General strategies which work in as many market conditions as possible this is because the market is non-stationary so it changes and perhaps the algorithm May uh may not work on a specific stationerity another problem connected to this is simulating realistic markets this becomes more important as we talk about machine learning because uh uh you don't want to train your model directly on the markets right you don't want to trade while you're training you you want to use a simulation of course you one of the best ways is perhaps to use historical data so create a simulation using a historical historical Market movements but there are things that you can Sim you can't really simulate for example Market impact okay so this was just a general overview let's um let's now try and understand how financial markets work in a very intuitive fashion so the fin markets and the price process of financial assets are generated by the interaction between many different Market players perhaps also many of you have have invested and have participated in the price formation process of a specific asset but today I wanted to focus on the main say Financial actors the main participants in terms of volume in terms of how much they trade and I've identified the portfolio managers the Traders and the market makers now often times the lines between one profession and the other is quite blurred but for today's purposes let's try and give a somewhat specific definition and so we have the portfolio managers which are investing the the liquidity of clients it's usually we're talking about large amounts hundreds of millions if not billions so very large sizes which means also uh low low frequencies okay then we have the Traders or even called prop Traders these are usually found in Prop Shops or even the prop trading areas of Banks and they're also deciding the trading strategies but they are investing the liquidity of the firm for which they work for and they're often assigned a risk budget so they don't have just let's say cash but they have a risk budget and so they operate in derivative instruments usually using a lot of Leverage and they work at an intraday frequency some often they tend to close their positions end of day and this is the type of Market participant who we will be focusing on today um then finally we have the market makers the market makers are extremely important because they provide liquidity to the financial markets and portfolio managers and Traders I mean when when they they execute when they invest they will need to find someone on the opposite side willing to do the the opposite trade and it may be possible to find another portfolio manager or another Trader but it's not always the case and so market makers are there to to help uh with this liquidity and then finally we have the the financial markets the financial exchanges the there are regulated financial markets such as the New York Stock Exchange now London Stock Exchange where here we what we can see is all the limit order books so all the orders which are being posted and we will see more in detail what I mean about about this then we also have a overthe counter type of instruments and which can be traded via chat or the more standardized ones also through multilateral trade facilities and then there's also exchanges venues like dark pools which are used if you want to minimize Market impact as the the liquidity is is hidden so this was an overview of the financial markets let's now uh see an overview of the Technologies which are used in this field and so let's focus on the algorithmic branch the first uh Division I I'd like to make is between rule based and Quant finance and uh when talking about rule based I'm thinking of hardcoded strategies let's do a simple example let's say I we have two moving averages a 30-day and a six Monon moving average and when they intersect I buy or sell okay a rule based strategy then we have Quant Finance which I would further divide into classic and machine learning and in the classic qu Finance I'm thinking of the mathematical Finance world so let's think of black and shorts so we uh assign a stochastic process usually BR a motion to the the asset I'm interested in then have my objective function and I want to find an analytical solution we're essentially in the field of stochastic control and then instead we have machine learning which are datadriven models models that want to learn from experience and of course there's many different machine learning techniques here I've I'm just showing the ones which we will be focusing on today so we have a supervised learning reinforcement learning and expert learning okay so this was just a a warmup this here we can see how the rest of the presentation is going to go so I we will now be going through say machine learning tool kit so that we all have the basic basic intuition of the machine learning tools which I will then be using to exp and showing how they're used in the quantitative trading so in the in the second chapter and then finally at the end we will see a a use case of a real world example of of machine learning in in trading okay so let's begin with the machine learning introduction and uh in machine learning the there are three main paradigms we have supervised learning unsupervised and reinforcement learning and supervised is the the most well-known and the most widely used it's uh mainly used for regression and classification um in general I would also let's say classify deep learning in in this in this part then we have unsupervised learning which is used for clustering dimensionality reduction feature extraction um think of the K means algorithm and then finally we have a reinforcement learning which was born for game playing for optimal control it is used to learn sequential decision making processes and uh we will be talking a bit about the about this family of algorithms today so let's um let's go in a bit more detail of of how reinforcement learning works and so the basic building block of reinforcement learning are Mark of decision processes which are processes that describe an interaction between an agent and an environment let's already um let's already project ourselves in a trading in a trading example okay so imagine that our agent here is an artificial Trader and the environment are the financial markets so the agent is going to receive information from the environment through the state variable so it's going to have Market features so the prices of the asset you you want to trade in and internal features so the current position of of our of our Trader of our portfolio and then the agent will take an action so can decide the new portfolio position it wants to hold or the new trade to make and then it will receive a feedback from the environment which is a pnl profit and loss minus the cost the transaction cost it paid for for the trade and this uh happens iteratively okay so the objective in this field is to find a policy usually refer to as Pi which is a function that Maps states to actions so the object is to find a policy that maximizes the discounted sum of the rewards so this J we want to find the policy that maximizes this J and uh how does this work imagine that initially our so our agent is for example in real Network and as always at the beginning it's randomly initialized so this means that it will be doing random trades but what we want is that as it uh tries different actions and collects experience we want this this policy to become optimized in order to maximize this objective in order to uh in order to achieve this learning process while we use reinforcement learning algorithms and uh there's many there's many reinforcement learning algorithms there's uh a vibrant research Community which is continuously updating and uh finding new new approaches but I would say that the two basic families are Q learning and policy search type of algorithms and in Q learning what happens is that we want to learn the Q function this Q function is defined as the expected value given that we're following a policy of the discounted sum of the future rewards conditioned on a specific State and a specific action and so in order to transform this to a policy what happens is that if we have our Q function once we're in a specific state what we need to do is pick the action that maximizes our Q function now how do can we calculate this Q function if if we don't know it well we can use the Bellman equation which is this recursive formulation which enables the iterative calculation of the Q function as we gain experience so as we interact with the environment and the B one equation has has um given um inspiration for the Q learning algorithm which is perhaps the most well-known algorithm in this field where essentially you're changing this expectation operator with with maximization operator now as you can see this this Q learning algorithm is a table algorithm but it can be generalized also to work on continuous State and actions by using function function approximators such as XG boost or nor networks then uh the other family is the policy search uh type of algorithm and uh here we're not interested anymore in the Q function but we want to learn directly our policy so we Define uh with a parametric policy which can be a neural network and then we update its parameters through a gradient ascent and we're moving in the direction of the gradient of our J of our objective function and this can be calculated by using the policy gradient theorem and now there's also um approaches which combine let's say policy search and Q learning which I refer to as actor critic okay so now let's go on to the next uh type of algorithm which is referred to as expert learning this is perhaps a bit less uh less known but it is a field of research close to reinforcement learning in the sense that we're still working with sequential decision processes and uh one uh uh one big difference let say is that these algorithms are online and do not need a an offline trading phase to start and uh another difference is that there is no State space let's let's see this interaction scheme to understand a bit better what I mean so we have our we still have our agent interacting with the environment okay and the agent picks an action the environment gives a feedback um in reinforcement learning we talked about reward words here in this framework we're usually referring to losses but the concept is the same now in order to pick what action the agent wants to take the agent is going to receive the the suggestions of a number of experts okay and so it sees this feedback from the experts and then picks an action which can be a weighted average of the suggestions of these experts in this field the objective is to de Dev algorithms which are capable of converging which are capable of understanding which one the best expert is in sublinear time this is referred to as having um as having regret guarantees regret is defined as we can see here in the slide as the accumulated sum of the losses of our agent minus the accumulated sum of the losses of our best expert and so the objective is to have a sublinear regret now let's see an example of an expert learning algorithm this is also a research field which is uh has a new new papers new discoveries every year this is just one example but there's many others so and also in this case let's project ourselves in a trading or actually more precisely in a portfolio optimization framework and let's assume that our experts are are strategy our trading strategies how can we use expert learning so we start with an equal weighting over our our strategies we have M strategies here so our weight our weights are are the same one over M um we also have a learning rates which we need to pick and then we start with our iterations so at each iteration we collect the predictions of the experts we play uh our agent place this the weighted average of the predictions of the experts and then we can observe the loss the feedback from the environment so the loss that each expert would have achieved and then we update the weights with this new information so with these losses and this is the update equation of the of exponential weighted average very simple update as I mentioned there's many other algorithms some even with more complex formulations for the updates and what changes between the different algorithms is that they have different regret guarantees okay so uh one last concept on the machine learning side I wanted to mention is that of hyperparameter tuning so this is our data set okay we want our data set which is sorted by by time and we need to Define our training on our validation and our testing set how how would I use this so uh what happens is that we are going to train our model changing uh trying out different parameters if we think of a of a neural network but if we think also of a reinforcement learning approach all of these approaches have lots of hyperparameters and so we want to find the combination of hyper parameters which works best for for our specific problem in order to do this what you do is you try a combination of hyper parameters and you you train it with that combination and you see how it works on the validation set you then should pick the combination which optimizes your the objective function that you're looking at in your validation and only at the end you will verify how this uh this strategy performs on a test set which has not been seen before at this point if the strategy performs well the test set then you can can decide to keep going otherwise you should just discard the strategy and and start over uh also another thing to mention is that in the case of the expert learning we don't have the training phase but we will still have validation phase where we need to tune our hyper parameters and so how can we do this hyper parameter tuning there are three uh say three big uh three main ways of doing it there's the grid search where you just do a grid over your parameter space and you try every combination then actually random search has been where you pick uh randomly the parameters have been proven to work better but this is still not sufficient because when we're talking about uh like deep learning and supervised learning these algorithms take a long amount of time to train and uh uh so computational of course you can just get a huge stack but oftentimes you need to balance a bit because you have a limited amount of computing power and so what's what is used now is the so-called basian search there's a there's several libraries now open source libraries which can be found in our sited here Bean beijan search which essentially is a smart type of hyper parameter search which moves in the direction of the most promising parameter combination and so it reduces drastically the amount of times you need to call your training function and so the amount of time that this um parameter search can take Okay so let's uh let's now uh start with the quantitative trading chapter and let's start by defining what we mean by quantitative trading so it's actually a very Broad and generic definition it just means that we're using mathematical or statistical models to try and identify trading opportunities and if if you look online perhaps the these are the main strategies which the most common strategies so momentum strategies where essentially you're saying that you want to invest in those assets which recently have been performing better mean revers is a bit the opposite idea so an asset has been performing well then I assume that it's going to revert back to its means so I probably will want to uh sell it statistical Arbitrage it's usually you're looking at the relative value between two highly correlated assets and you assume that this this difference will be mean reverting then you have seasonality approaches where you are um you're assuming that there were there's going to be recurrences perhaps at a yearly or monthly or even intraday frequency and then you have Market making algorithms which behave like a market maker would so they uh they're trying to maximize the number of transactions they make by putting in the bid and in the ask side so our objective today is to understand how to build an effective quantitative trading strategy and to do that I would follow um let's say this this list starting from defining the objective what financial asset do I want to trade in is it what type of financial asset is going to be stocks is going to be bonds is it going to be Futures okay and what frequency do I want to work at do I want to work at a uh daily frequency monthly frequency or is it going to be intraday and then the style is going to be long only long short okay once I've decided my objective we need to download the appropriate data so are we're going to use just price data are we going to use the limit order book am I going to be using a fundamental data economic news okay I have my data set let's now build the trading strategy and this is the part we're going to be focusing on the most today once we have our trading strategy we can test uh we can test our strategy on First on historical data out of sample historical data then uh perhaps you're still not convinced that you can do some live tests let's say on paper money and then if finally you're happy with your strategy you might want to upload it to a server and connect to the market via apis so everything is automatic and U and you can just relax while your strategy generates a profit for you but so let's start now with a with a simple example a rule-based quantitative trading example and uh here we've decided that our asset is going to be the Euro doll effect X and the frequency we're working at is a 10-minute frequency and the the data that we're using is only price data this is our trading rule it is a mean reverting rule because um as you can see the r are the returns we have this minus here so if the return is positive our our positioning is going to say is going to go short so essentially we can see why this is mean reverting and then another parameter here that we have is this big T which is the the time Horizon now if we can calculate an expected pnl from this trading Rule and this comes out as this expression that we can see here and uh what what it's saying is that if this our time Horizon times the one period variance which in this case one period is 10 10 minutes minus the variance of of our time Horizon T is uh essentially if the this part is positive then we're going to make money there's actually there's also this mu^ squ which is the return of our asset so this expected pnl is saying that if the asset is moving in a range with a high one period volatility and a smaller let's say t period volatility then it's going to be profitable and the as we can see from the p&l on 2021 of this back test on 2021 this is accumulated p&l over time we can see that it's actually working pretty well but here there's actually there's one major thing that we did not consider and this is the transaction costs let's understand what we mean by transaction costs here we have an image of the limit order book we have uh all of the bid orders so the orders ready to buy on the left and then on the offer or ask side all the orders ready to sell and uh we can see for each price level the volume outstanding and we we will talk more about the limit order book but this is just to give a first intuitive understanding now a an important quantities here are the best bid which is the highest bid and the best offer which is the lowest offer from these two quantities we can calculate the mid price which is the halfway point between the these two prices and we can also calculate the spread which is the best offer minus the best beat now we can Define transaction cost as the trade the trade size so what we're trading times half the spread so the distance between the mid price and the bid or the offer it's it's symmetric and so at each time step our p&l becomes the position that we currently have times the market movement minus this transaction cost now notice that this assumption here is still uh I mean it it is restrictive in the sense that we're saying that we can only trade with a size that is inside either the volume of the best bit or the best offer okay so now we introduced transaction cost and we can see that our our strategy our p&l degrades significantly as we would expect and this is also this is especially because we're working at a 10-minute frequency so each 10 minutes we're doing a trade this these costs slowly add up and eat up on our performance but now the question is how can we improve because uh this this mean reverting strategy I mean it works it works kind of well on the euro dollar because it's it's a mean reverting asset right it's usually around 1 1.1 1.2 too and um and but how can we we want to find something more generic that can work also on other assets and uh we want to find something that optimizes also for cost because here in this trading rule costs are not considered at all so in general how can we move on from a strictly defined trading Rule and this is where we can Leverage The Machine learning toolkit which we saw before to try and overcome overcome these challenges here we can see an endtoend workflow to build an intraday trading strategy using machine learning and we're going to go through each component in detail one at a time but just to start by giving an overview um here I've um identified three main components we have this first component which is in charge of doing feature engineering and Signal generation so predictive signal generation can also refer to as Alpha signal extracting signals from the limit order book so in this case um if you remember the the initial list we want to work on limit order book data at an intraday frequency then we have the second component which takes the alpha signals and the the features and the market information and uh uh generates a trading strategy okay finally we have the execution component which has the objective of trying to improve the execution so try and reduce those transaction costs and the market impact so let's uh let's start looking at each component in detail and here we see another representation of the limit order book this is a graphical representation on the x-axis we have the different prices on the y- axis we have the volumes so in blue here we have the bids in green we have the asks and you can see the quantities which we defined earlier so what what we can see is that first of all the prices are discretized and they are distanced by one tick which it's called one tick and it's it's always um it's standard and uh this is we imagine that we have a so we need a data set to work with right this graph this graphical representation is one single row of our data set of our limit or book data set and so it's actually it's 40 columns where we have um the best bid price and the best bid volume then the second best bid price and the second best bid volume this for 10 levels on the bit side and 10 levels on the F side so that makes uh 20 volume points and 20 price points but usually the price points are not very informative apart from the best bit and the best ask as they're all distanced by just one tick and uh this uh so this is one row but this row gets updated each time there's an event in the limit order book and what types of events can there be well for example you can add a limit order if someone adds a limit order here with the price of 100 what happens is that the volume that he wants to trade goes at the back of this que and actually each queue here works as a first in first out cube so just to add a bit more detail what types of how can we interact with the financial markets so the basic type of order is the market order where we just specify how much we want to trade and the direction byy or sell and it's going to be executed at the best available price then we have the limit order where we are specifying the amount the direction but also the price so this was an example and then we have the cancellations so if we have inserted a limit order we we can it's going to stay in the limit order book unless it either gets executed or we decide to cancel it okay and so this limit order book data set is one of the two types of data set which you you can uh let's say buy from from data providers the other type of data set is the trades and quotes data and this is a smaller data set as it contains only the best beit and the best ask but it contains an additional information which is the executed orders so all the market orders that uh that have come in and also in this case each time there is a change and either the best bit the best ask or there's a new market order coming in we will have a new Row in in our data set so now we have uh We've downloaded our two data sets our limit order book and our trades and quotes data set what can we do so first of all we need to do some data quality right we need to handle the not numbers the gaps we might want to try and merge these two data sets even though uh at times the time stamps might not be exactly identical so there's a you need to be careful there and so be careful with these datetime features and then since we're working with Futures we need to handle the roll dates what do I mean by this well Futures are instruments which have an expiry and usually the most liquid future is the one which is closest to expiring uh so what happens is that when you're getting when you get to the expiry date you need to change to the next future and usually liquidity changes from the front contract to The second contract two or three days before it it depends on the asset and so you need to understand when this change in liquidity is and transfer from one future to the next and Okay so we've done our data quality now we need to take an extremely important decision which is what frequency are we going to work at okay and uh we can for example say we can work with every single row so tick by tick but that's that's quite challenging and it even may have say too much information so usually what most people do is they down sample and there's two main ways of down sampling you can down sampling by looking at how many ticks or how many events have happened with the idea that 10 events um when lots of things are going on are equally important as 10 events When there's less things going on and so uh you can look at the number of uh the number of ticks then the other type of down sampling is instead just looking at the the time so I resample every 10 seconds or every 10 minutes and this all depends on what frequency you want to try and work at so once you've decided the frequency we uh we can now start creating some features and and uh here I would like to distinguish again between classical approaches and machine learning approaches classical approaches well there's a lot of studies on Market micr structure you can try and generate some features such as the outo ciance of the of your price or there's lots of study on the order flow in Balance which was introduced by by r a few years ago or there's even the volume imbalance so you're looking at if there's more volume on the bid or on the X side or the trade imbalance you want to look whether there's more buy orders buy Market orders or sell Market orders and these are just some examples there's a long list of of features that you can find in the financial literature and then there's the machine learning approaches uh here I'd like to mention the Deep lob paper by by Zang um which uh is one of the first that introduced the use of convolutional neural networks specifically designed to extract microstructural Market features and now it's becoming quite popular another approach you could use for example instead to try and just compress your limit order book information by using an outo encoder so here for example we can see how we with this out encoder we could compress the bid side or the X side which has 20 uh 20 data points to just to just four Okay so we've we've now defined decided which features we want to use and how we want to tackle it we need to Define our Target for our predictive problem because we want to predict the price um so let's let's understand what type of price we're talking about and you could of course frame it as a regression problem but that's uh that's going to be extremely challenging what what usually happens is that you define it instead as a classification problem with three classes you want to know if the price is going to go up if it's going to stay stable or if it's going to go down and so you're going to compare your mid price your current mid price with the mid price that's uh that you're going to have in X seconds or X ticks depending on the frequency that that you decided earlier and and then you you need to decide a a threshold for which you want to Define these three classes your Theta but here as well you can also um think of other approaches in the sense that um maybe you feel like the mid price is too noisy and too volatile as maybe it there was just one small jump and then it came back down and so it's not something that could be tradeable and so there's also um research papers that instead consider a rolling average price so I don't know the the average price between the next 10 and the next 20 seconds or another another possible thing to do is also to instead just focus on the bids price or the a price instead of looking at the mid okay so now you've defined your target you need to choose your your classifier um deep lob we've already mentioned it it's uh it's becoming quite popular it uses a CNN combined with an Inception layer and then an lstm but I mean of course you can Define your your structure as you deem more appropriate but you also have tree based methods which have um achieve great results in classification so XG boost lbm extra trees and if you're undecided you could even just trade more than one and then do an ensemble also to make your result more robust and let's remember that you you may want to have your hyperparameter uh tuning procedure already implemented because it's a all of these algorithms have quite a large number of parameters which which is going to be important to tune and here we can see an example just graphical example of the confusion Matrix that we will be analyzing once we have our our results and um so we have our predicted labels our true labels what we want to do is maximize uh let's say the the green boxes and minimize the red boxes while the blue the blue boxes are let's say a bit less important what do I mean so you want to be really as precise as possible in predicting for example when it's going to go down predicting when it's going to go up but you don't want to confuse that it's going down with the fact that it's going up because that will certainly cause a loss if you confuse down with stable that's that's not ideal but at least it's less dangerous than confusing down with up and uh and so all of all of these things that we've just seen are ideas and intuitions which are expressed in several several papers these are just some examples of some reference of some references of research groups working on shortterm forecasting by using the limit order book okay so now we have our predictive signals we have uh our features our Market features how can we transform this into a trading strategy of course you could say okay my prediction says the price is going to go up I'm going to buy my prediction says the price is going to go down I'm going to sell and it's if it says it's going to stay stable I'll just stay flat but actually I mean the there's tons of different combinations you could come up with right different rules and you also need to remember that there's transaction cost it's time you do an operation so a good method uh could be to use reinforcement learning to learn this task and uh so in in our state we're going to show the trading signals that uh that we've uh that we're working with the market market prices are featur and the current position of our portfolio the action can be uh for example the the current portfolio or the trade here long short flat I'm thinking of the current portfolio Okay so if uh one action is long and then at the next time step the next action is short this means that the trade is going to be double the size than going from long to flat and the reward is going to be the pnl minus our transaction costs and so here I mean we have to do is of course choose the reinforcement learning algorithm um you can choose it by using your experience or just with an experimental approach so you need to choose if it's going to be a q-learning type policy search type or you could even do an emble model here as well it's very important to have your hyperparameter tuning approach because these reinforcement learning algorithms really have lots of parameters which need to be tuned and uh so finally ideally you to obtain your trading strategy so the next step is that uh your trading strategy is doing well but perhaps it's um the execution costs are the trading costs are quite high and you want to try and lower them or in any case you want to try and increase the amount of assets under their management and so optimizing execution is is important and uh how can we do this also in this case we could use a reinforcement learning approach where in the state we need to insert the trade we want to make if it's going to be to buy to sell how much the execution time our Market signals can always be important the market information and uh uh the the execution strategy in example could be to decide whether to execute immediately to post a limit order or to execute at the end let's let's do an example to to understand a bit better how this type of algorithm could work um so let's say that our trading strategy gives a long signal so it says go long because it's it's working at a 10-minute frequency and the 10-minute predictive signal predicts that the price is going to go up in 10 minutes but then we also have a short-term signal which is the one that we're giving to our um execution algo which is a 30C look ahead signal which says that instead in the next 30 seconds the price is probably going to go down by a couple of ticks and our our agent could decide to buy by posting a limit order maybe one tick below the the current the current price so to uh achieve a slightly better execution and shave off those those transaction cost if instead also the 30 the 302 um prediction was saying that the price is going to go up it may be more convenient to just execute immediately before the price before the price goes up and so here now we see our intraday uh we see again the the entire workflow and now hopefully each component is is more clear but so the question is are are we finished now can can we be happy relax and go to the beach and let our strategy do the work for us but the answer is probably probably not running a single strategy may not be sufficient because I mean the market is not non stationary and um perhaps something happens the FED changes its monetary policy and so we start a different stationerity which the our algorithm has never seen before and so we start slowly start losing our strategy starts losing and uh at a certain point we we start going negative and then we get worried and our management gets worried and you shut down the strategy and maybe if you would have waited a bit more the strategy would have recovered but um but it's always it's always difficult to understand how to behave in those situations um another another issue could be that since we've assumed that we're only working with the bid best bid and best ask for transaction costs we have uh Limited amounts of assets Ander management which we can run on on our strategy and management once us uh wants us to push on the asset under management so we need to increase and how can we do that well we can uh learn and run more strategies because now we have all of our workflow and all we have to do is let's say choose a new data set and run it through a workflow to obtain uh another strategy but so how can aggregating strategies help and here we can see how it helps in terms of sharp ratio if we have our two strategies with returns mu and and volatility Sigma we can see what happens at if we run these two strategies together what happens to the sharp ratio and here was fundamental is correlation these strategies need to be uncorrelated because as we can see in the table if the correlation is high we have this expression where we're summing on the numerator the returns and the on the denominator the volatilities well if the correlation is zero we can see that this square root comes in which um which means that actually this this sharp with zero correlation is is higher than this sharp if correlation was one of course if correlation becomes negative uh things could improve even more but then at that point probably the returns would be canceling out okay and so what we want to achieve is essentially an algorithmic multi Strat type of um type of approach where we have lots of uncorrelated strategies running so possibly on on different assets and then they are weighed so you have a so-called Central risk book which decides how much assets under management or how much risk budget to give to each each of your strategies and so the question is how can we combine these these strategies and so there's uh three main steps uh in order to combine them I would say the first one is to choose the rebalancing frequency and uh so our original strategies are working at an intraday frequency looking at the limit order book perhaps at the second level but uh when rebalancing we don't want to work at this frequency we want to let the strategy run and observe how it's working and then try and give weights to those strategies which are working better and so what becomes important also from a more practical point of view is when you close your positions and if they intraday strategies you're often closing your positions daily or at least weekly and so depending on how you're closing your positioning that's when you will want to rebalance if you think about it from a practical perspective um if your positions are are closed I mean if you decide your weights once the positions are closed all you have to do when you start over when you reopen them is that each strategy has the appropriate weight well if instead you want to change your weights while the strategies are running you have to correct the trades that each strategy can make and it becomes a bit a bit more complex but so before uh deciding how to weigh our strategies though we need to make them comparable and to make them comparable you could simply just divide the the positioning of the of the strategy by the the volatility the p&l volatility generated by the strategy and so all of them will will have essentially the same amount of risk and you can compare them and so now how do we choose the weights that we assign to each straty you could assign them equally okay that would be the say the the base case is to just assign the same weight to each one you could use mean variance optimization but uh we are say machine learning experts and so we want to use our expert learning approach and so actually now we're shifting from behaving like a prop Trader to being a a portfolio manager right an asset manager which needs to decide how to allocate its total funds or its total risk to a set of strategies in order to have a balanced exposure in order to minimize risk while maximizing returns and in order to diversify and our solution is to use uh models which avoid the forecasting steps are adaptive to Market and don't need a large training set and this and the expert learning approaches which we introduced before we saw before we saw exponential weighted average but there's also other approaches exponential gradient onlineon step as well as many others these certainly take take these boxes and so we can we can use these approaches in order to weigh our strategies all right so to recap this chapter um this is this is what we get we have uh we want to create n different strategies where each strategy is um is generated by using the three components that we saw feature engineering and Signal generation strategy generation and execution then we want to normalize these strategies so they are comparable and then aggregate using expert learning so let's now go through yeah yeah so we have few questions so would you like to answer this now or after the use case uh the use case just going to take three or four minutes so I'll finish up and then uh we can go to questions sure that works thank you yeah thanks and yeah the use case is a bit of a simplified version of what we just saw what we saw is say still work in progress and uh here we're focusing on uh the second component and so we're using reinforcement learning to learn the trading strategy and uh this is the usual Mark of decision process so our action is going to be just three actions long short and flat in our state we're going to have the Market info in the current portfolio and the reward is pnl Minus cost let's so we're at an intraday level uh we're working at a minute frequency and uh here we we're training using a reinforcement approach on historical data 2018 2019 we did our hyperparameter tuning using as validation set 2020 and this is what we see as a back test on out of sample data 2021 now we can ask ourselves what um what has the algorithm learned what type of strategy is it is it running and here we have a heat map that shows the days of of the year and the time of day on the x-axis and we can see that uh this is pro possibly closest to a seasonality type strategy an intraday seasonality as the similar actions are repeated every day showing that perhaps the time of day is a very important feature but at the same time they're not exactly the same and uh so showing that of course also the market Behavior has an influence on the trading strategy and the so then the next step is how can we improve on this and this is the part where we aggregate multiple strategies so um here we have all of our experts let's say all of our strategies which are these um dashed lines this is the accumulated pnl on 2021 and these strategies are all different actually they're on on the same asset but they're different because they've been trained on a different stationarity so different period of time and so they are also let say uncorrelated to to each other and uh uh on the right we can see how the expert learning algorithm how our aggregator is assigning the weights and the colors coincide okay and actually the the blue highlighter here is our combined strategy so what can we see we can see how the yellow strategy which is performing well around the the middle of 2021 get achieves a high weight we can see how this light blue strategy which underperforms is underweigh and uh uh tends the way the strategy disappears um then we have this uh dark blue which is instead the one that ends up with the highest performance and we can see how it tends to achieve a higher weight by the end of the period so essentially we can see that uh um it's doing what we would expect giving more weight to those strategies which are performing better okay and uh that concludes U that concludes the presentation so we can go with the questions but if we don't have time to go through it I'll feel free to contact me by email or LinkedIn and uh we can chat on there as well okay thank youo for your instance session I'm like I'm like I learned lot of things new and I hope our audience also learned uh a lot of the things new and also few things in detail um so thank you very much so uh we have few questions so we will go through that uh first few online questions and then followed by in person question okay so uh this is a question from ramanathan that how do you model out layers like breakouts um so outliers uh you want to probably handle that at at the beginning in the when you're doing data quality I would uh I would make sure that my data set is um is free of possible outliers or gaps or anything I mean that I would take care of that at the beginning at the the at the start yeah okay so the next question is what all uh do you predict the price or the range Gap up gap down yes so maybe we still hadn't gone through it but the this is the Target right so this is what we're predicting we're so the class yeah yeah okay so uh we just going to predict the whether it will go up or stable or down basically yeah the class exactly yeah so what kind of feature engineering do you employ I think you have already covered that but would you like to cover it yes sure for so when uh generating features as I said you can go through a more classical approach where you look through the um micr Market microstructure literature and you can Define certain uh certain Market features or you can use the machine learning approach where for example using convolutional neural networks um appropriately configured um can uh has been proven to also generate similar features to what have been found also in the in the financial literature and is there any um training of these expert models is involved training or retraining time to time the based on the auto variance uh so training uh it's a I mean yeah it's important to to retrain your model when you have a a good amount of of new data so to to keep it updated uh of course I mean I in my experience I'm not doing it too often and I'm talking about perhaps six months later you could you could do an entire retraining but um yeah it's usually based on when you have enough data to to learn something uh different and so it makes sense to redo the the entire procedure from the start okay okay thank you and uh what are the typical uh so what are the performance metrics or Benchmark you uh you follow to assess it any Benchmark uh yeah benchmarks are tough to so and of course there's several there's several levels right so if when you're looking at prediction I mean as I said you want to have a high prediction that it's correct prediction 50 60% but it depends also a lot depends on what frequency you're working on what Target you're using I mean the higher the frequency the better the prediction should be the lower the frequency the less uh the less good it becomes and uh also if you're using uh rolling if you're using average prices you expect different results than if you're using just a mid so there's a lots of uh things that that can change in terms of the reinforcement learning case here there's not really a benchmark what what I do is I look whether the strategy makes sense uh I mean first of all whether it's profitable on the out of sample and then it also look at the uh the heat map that we saw earlier to try and understand if it's du doing something that makes sense or if it's just completely random and then finally when talking about the expert learning part a simple Benchmark is just one over n for example and uh and you want to be better than just equally weighing all your strates okay so now we have a few question from Christian Q so how would you suggest to tackle the volatility of stochastics policies in re enforcement learning to improve accuracy um s just to tackle the volatility of uh stochastic policies like I mean uh you of course there is a I guess that's asking whether I mean the policy is can be stochastic and so yes you could try do ensembles right I mean you have you could um learn actually even this approach is something that helps to reduce the stochasticity of of the policy because here you're you're running multiple strategies and and then you're weighing them and so in some sense you're you're weighing out the fact that that the policy is stochastic um so yeah some sort of Ensemble of aggregation of of more strategies all right okay I think we have um okay so uh we have another questions uh hang on let me call out black one yeah so this is from tan that uh how do you uh handle a black one events H strategy yeah yeah that's that's good question and that's actually that's uh something that you can't really hand let's say with with these approaches because these these approaches here learn tend to learn things which happened perhaps on average in the in the past okay and so if there's something new uh which hasn't happened before it's a I mean it's just a coin toss at that point whether it's going to guess it correctly or not and that's why also you you want to combine strategies right because maybe one strategy is going is not going to guess correctly but others will and so it kind of averages out but if uh if you're working with black one event I mean this is not the approach to work with black one events you you would want to use other other approaches buy options or I don't know do some manual manually shut off the strategy but yeah this type of approach is not built to predict black on okay okay so I think you we already covered it but like there are like a a slight different question like how do you handle like unnecessary uh breakout economic data releases and Forex session changes like the during the Forex session Asia Europe and us uh how do you handle the when there's economic releases like the the FED policy stuff like yeah that's a similar concept to the the Black Swan in in this that yeah it's it's a coin TOS right you're not you haven't predicted them I mean you're not working on predicting the data that's going to come out you could try and introduce that that data point as a feature okay so when you're doing um here we only concentrated on limit or the book but as we saw at the beginning I mean you when deciding what type of strategy you want to you want to create you could uh you need to decide what data you want to know you want to use and you could try and insert microeconomic data as a as a feature and perhaps what you want to do is you want to have uh another separate algorithm which tries to predict how the data point is going to come out and then you give that prediction as a feature to to your algorithm but yeah from from what we saw today I mean I didn't present this part but that's how I would I would tackle it okay uh so we have another question uh how the encoded is used to extract uh feature in Lov is the model using all the Lov for a given day or it's just like the bid and ask are feeding to the encoder no so I would use the entire limit order book for the for the first part um I mean yeah as we mentioned you could even you can either create micr structural features and from the entire limit order book or you can give the entire limit order book to a convolutional network but here yeah I would be using the entire limit order book then perhaps you could say okay but my model is becomes too too heavy because we have all of all of this data so you could do like feature first you try once with the entire limit of the book you do feature importance you see which features are more more important and so you can then reduce the the model size and work with smaller models all right so thanks Ado uh so now we will take a inperson question from our live audience so uh whoever has question can come around because I don't have speaker so maybe anyone have please yeah yeah hey hi Edo thanks for the Fantastic session I just had a question on this expert learning so I'm not you know super familiar with this I mean we worked with some reinforcement learning algorithms for trading in the past but this I was just wondering this expert learning basically uh is sort of learning in tandem right like we trying to sort of imitate the expert strategies right can this particular framework be recasted as adversarial strategies as well like something that you want to avoid rather than mimic uh or perhaps like you know recast it as a competition is there a possibility like you know sort of uh would that be something which is kind of closer to reality like you know competing trades or perhaps bench marking it to certain index uh or something like that so that's my first question I do have another question any sort of thoughts on this yeah so in this expert learning what it's what it's doing yeah it's it's it's trying to understand essentially which one the best strategy is and yeah it's it's adapting at every time step right it it updates it weits it adapts and it tries to give more weight to that strategy which is which is working best and there are some I mean some of these algorithms are created with adversarial assumptions in the sense that um you assume that the environment is an adversary and uh so that you want to have the you want to come up with the strategy that is able to say confront an adversary environment but I'm not uh I mean it's not I think you were referring to having the adversarial experts in some sense I so I'm not uh yeah the expert sort of playing the role of the right but I get it like you can sort of model it within the environment but perhaps the experts remain experts and not aders yeah yeah I mean it's it's built on the fact that you you can trust your experts and you just want to understand which one the best is out of out of these but yeah I'm not sure model them as adversaries yeah unless one of the experts goes wrong but that's all right so uh another question is uh to do with this explainability right uh let's say your cumulative pnl is going in the red is one able to clearly then list out sort of you know the features that explain that particular performance uh that you're observing you know the red pnl is it easy to kind of yeah just trace it back uh the red pnl to U your features I mean is there a natural feature importance metric that comes out of this um no no it's a that's that's a challenging part I mean you don't um at this uh with this framework you don't have too much explainability I mean as I said this is this is something that you can try and explain and show right you look at this heat map and it it makes some sense it's but U you don't uh and another thing you could do is do feature importance to understand which features can be more more predictive so so perhaps your feuture importance says that volume imbalance is very predictive and so it it has a lot of of importance when choosing the action I mean that that's what you can do but if if you're it's hard to say my p&l is going red because of this feature I mean you can say which features are important when deciding the trade to make and you can see kind of what it's doing but then I mean more than this uh it's a it's really challenging I mean for feuture importance there's there's things like shop right the shapely values or if you're using trees they have an innate feature importance but yeah that that you have to work with with this type of information is hard to extract more more detail I mean you you don't know I don't know if you can't say yeah because the FED is tightening then this uh this strategy is doing this type of behavor Behavior I mean you don't have that type of insights so thank you but maybe I'll just plug in an advertisement here so I do have a paper from about 10 years ago uh in Quant Finance Journal which basically does this whole you know realtime monitoring of the automated trading portfolios wherein on an online basis we're trying to figure out what are the market factors that actually uh can explain uh the extreme pnl so that was a paper with Charles Lal who uh currently at who was at whatever Capital funds management but is now at Abu Dhabi investment fund but but yeah thank you so much that's thanks please please okay anyone have any other questions okay so in meantime I I have just one question about the feater importance so do you uh Implement any uh feature importance gap on it in order to like not to give importance on like one dimension or two Dimension maybe like it is having a cap certain cap so that it can give look into the other features as well sorry you say again the the Gap so basically so during the feature importance we see that how much weight it gives the model into the each of the feature right so do you have any specific cap like it should not give the model uh feature importance on like the more than whatever is required and then spread it out um honestly I no like usually you don't have one feature that's I mean from my experience I've never come up with just one feature which is too important there it's usually quite um homogeneous and then some just disappear but yeah I that's that's not happened that there's been only just one features that that's been too important usually the these agos try and use most features from from my experience at least all right all right thank you hi Eduardo uh so new to the field but basically what I wanted to ask is how are we like choosing the optimum frequency and um the size of data set considering the entropy of the market the optimal frequency is something that you should uh pick based also on what type of infrastructure you you have um in the sense that um I think that these algorithms work better on higher frequencies okay because there's more predictive power in the limit order book at at higher frequencies but often the the difficulty is trading at those frequencies because then um you need to have a really a low lat see infrastructure and so in general my suggestion would be to try and stay at the the highest frequency which which you can handle with with your infrastructure essentially that that should work best with these type of approaches Okay and like the data set size because I can see in the example it's like two years of data for training and okay uh yeah yeah exactly I mean I've uh so you have that also depends on the frequency right if the the higher the frequency I mean the more data points you're going to have for each day and um I've noticed that if you're working at a daily frequency uh sorry a minute frequency so like 1 minute 10 minutes having two or three years of data to train on is pretty good usually it's yeah two or three years to train and then one year for validation but if you if you go into higher frequencies I mean you can of course reduce reduce your training set because you're going to have lots of more data points during the day all right okay and my last question is whenever there's like a significant like a for example the lockdown that was kind of an anomaly in the thing right so the data we get from that do we kind of give it less importance more importance like data we get from any kind of like black on events we give it like less relevance during training or more like that's a that's also a good question and as usual the answer is a bit it depends I mean it's if it's something that appears only once like black one event if you insert it in your training set it might just add noise and so let's say the algorithm learns something on this event perhaps it learns to make a lot of money because of that event but then it's something that's Black Swan so you're never going to see it again and so you've you've learned something that works on that event but it's not going to be useful so my Approach but of course this is debatable is to um remove if is it's just a onetime black one event it's better to just remove it and learn the market when things are say more normal okay thank okay thank you thank you uh any any other questions all right so uh so so there is no questions uh so thank you ad again and uh first most uh I would like to thank uh grunow foundation for organizing this event across the globe so the whole idea is to bring all the intellectual and quantitative Enthusiast in a single single platform and discuss this kind of techical challenges and the events with us and uh and thank you very much Amit to be on the ground I mean you are you do a fantastic job and thank you bangalo for being here it's like uh it's really nice to see you uh here so thank you very much it was really great to see you all yeah and thank you Benjamin for helping me out with the all the logistics thank you for helping me out as well yeah thank guys for inviting thank you for taking your time so thank you again it's been great thank you guys thank by by bye bye bye bye
