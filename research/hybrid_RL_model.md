Great. I’ll begin an in-depth analysis and research on your hybrid BTC/USDT trading architecture that combines a supervised CNN-Transformer model with a reinforcement learning agent. I’ll focus on interaction between the tiers, optimal RL design, handling non-stationarity, feature engineering, risk integration, evaluation, and potential challenges—with actionable recommendations and citations from recent research.
I’ll let you know when the research findings are ready for your review.


# Project Minotaur: Hybrid Supervised+RL Trading System for BTC/USDT

**Overview:** The proposed system layers a CNN–Transformer supervised model (Tier 1) atop an RL agent (Tier 2). The supervised model ingests multi-resolution dollar-bar inputs and outputs predictive signals (e.g. probability of hitting a take-profit). The RL agent treats these signals as part of its state (alongside raw market features and internal variables) and learns an execution policy. Key design questions arise around how to encode the supervised output, choice of RL algorithm and state/action space, handling non-stationarity, feature engineering, risk controls, and evaluation. Below we survey best practices and research insights for each aspect, with citations.

## 1. Interaction & Information Flow

* **Encoding supervised signals:** The Tier 1 output (e.g. \$P(\mathrm{TP;hit})\$) can be fed to the RL agent as a continuous feature. It may be passed directly as a probability or log-odds (logit) to spread out values near 0/1.  One can also discretize it into bins or “signal states” (e.g. {bullish, neutral, bearish}) if that aids policy learning. In practice, the raw probability often works well, especially when the RL network can learn to weight it. Some architectures even treat the supervised model’s hidden features or attention vectors as additional inputs. For example, Fu *et al.* (2023) propose jointly training an LSTM predictor and a DDPG agent: the RL agent is initialized (“warm-started”) with the LSTM’s guidance to speed up learning. This suggests using the supervised outputs as **priors or guiding features** for the RL agent.
* **Balancing signals vs. raw features:** The RL state should include both the predictive signal(s) and other market features (e.g. recent price returns, volume, volatility, etc.). To prevent the agent from blindly following the signal, one can normalize or clip the signal, or let the network learn an “attention” weight. For instance, one could concatenate the supervised model’s hidden-layer embedding with the price features and let the neural net learn to down-weight noisy signals. Ensemble or gating mechanisms (e.g. as in Mixture-of-Experts) could also blend signal-driven and signal-agnostic policies. In any case, RL will implicitly learn how much to trust the signal based on realized rewards.
* **Feedback from RL to supervised model:** The two tiers can form a feedback loop. For example, the RL agent’s performance (and the market regime it encounters) can highlight where the supervised model’s predictions are less reliable. One can periodically retrain or fine-tune the supervised model on recent data or on examples where the RL policy made costly mistakes, similar to *hard negative mining*. This dynamic adaptation (akin to co-training) is largely unexplored in the literature, but conceptually one could use RL-derived pseudo-labels or residual errors to update the predictor over time, creating a continually learning system. In short, the supervised signal should be continuously re-evaluated against trading outcomes, ensuring the two modules reinforce each other.

## 2. RL Agent Design

* **Algorithm choice:** Suitable RL algorithms include both on-policy (e.g. PPO, A2C) and off-policy (e.g. DDPG, TD3, SAC) methods. On-policy methods like **PPO** offer stable learning and have been widely used in trading contexts; for example, Trovato & Tobin (2020) note that PPO is *“stable, fast, and simpler to implement and tune”* for stock trading. A2C/A3C are similar actor-critic methods if discrete actions suffice. Off-policy methods (DDPG/TD3/SAC) handle continuous action spaces and are more sample-efficient. In particular, **DDPG** (Lillicrap *et al.*, 2015) is effective for continuous controls (like order size). In practice:

  * If actions are purely discrete (e.g. {buy, sell, hold}), PPO or DQN variants work. If actions include continuous dimensions (like order size or price offset), use a continuous-action algorithm (DDPG/SAC) or a hybrid approach.
  * For hybrid discrete–continuous actions (e.g. choose buy/sell discrete *and* a continuous size), one can either treat it as a joint continuous output (e.g. output real in \[–1,1] where sign=direction, magnitude=size) or adopt a hierarchical policy. Feiyang Pan *et al.* (2022) propose exactly this for limit-order placement: a continuous agent first chooses a coarse “price range,” then a discrete agent picks the exact price tick. Likewise, one could use a continuous actor to decide position size (as a fraction of portfolio) and a discrete actor to decide direction (long/short/flat). Such hybrid architectures (possibly two neural networks or a multi-headed network) are supported in modern RL libraries.
* **State representation:** Use a subset of the multi-resolution features that capture relevant market information at different scales. For example, incorporate: *short-term bars* (seconds/minutes) capturing immediate momentum; *medium-term bars* (tens of minutes) for intraday trends; and *long-term bars* (hours/days) for overall volatility or regime.  Concretely, the state could include the latest 5–10 short bars’ OHLC and volumes, plus trend indicators from medium/long bars. Also include the predicted probability (or logit) from the supervised model for each horizon if available. Importantly, **private state variables** should be included: e.g. the agent’s current inventory/position and elapsed time within the trading episode. Nevmyvaka *et al.* (2006) emphasize that “state variables… can be derived from the microstructure data and the agent’s own actions,” divided into *private* (time remaining, shares left) and *market* (features of trading activity). Thus the RL input should at least contain the current portfolio position and time/depth of execution in addition to market features.
  *Example CNN–Transformer feature-extractor for market state.*  Pan *et al.* (2022) use stacked 1-D CNN layers followed by multi-head self-attention to encode high-dimensional LOB time series into a compact state vector. A similar CNN+Transformer block could be used to fuse multi-resolution bar inputs before policy layers. For instance, short-, medium-, and long-bar sequences can be concatenated and passed through shared CNN layers and a Transformer, yielding features that the policy network (actor/critic) then consumes. The figure above (adapted from Pan *et al.*) illustrates one such architecture.
* **Action space design:** At each decision step, the agent must choose an action like *Buy*, *Sell*, or *Hold*. In addition to direction, the agent could specify *size*. One design is to let the action vector consist of *(direction, fraction of capital)*. For example, output a continuous vector \$(a\_1,a\_2)\$, where \$a\_1\in\[-1,1]\$ indicates buy vs sell (positive=buy, negative=sell), and \$a\_2\in\[0,1]\$ is the fraction of funds to use. Alternatively, keep {Buy, Sell, Hold} as discrete and have a separate continuous branch for size. The hierarchical two-stage RL approach (above) is another option. The library choice impacts this: SAC/TD3/DDPG naturally handle multi-dimensional continuous outputs, while PPO/A2C can be set up with discrete heads or mixture distributions.
* **Reward design:** Beyond raw P\&L, incorporate risk and utility. For instance, one can use a reward that is a weighted sum of profit and a risk penalty (e.g. \$R = \Delta\text{PnL} - \lambda \cdot \mathrm{Var}(\Delta \text{PnL})\$).  Many works explicitly use *risk-adjusted* metrics: e.g. training to maximize **Sharpe ratio** of returns.  Huang *et al.* (2024) discuss shaping rewards around Sharpe or Sortino ratio to trade off return vs volatility.  One could also impose **drawdown limits** by penalizing large losses or terminate episodes when drawdown exceeds a threshold. Utility functions (e.g. log or exponential utility on wealth) are another approach to encode risk aversion. In summary, define the reward so that it encourages both profitability and controlled risk – e.g. combine excess return with a volatility or max-drawdown penalty.
* **Exploration vs. exploitation:** In non-stationary markets, sustained exploration is crucial. Techniques include ε-greedy or entropy bonuses (as in SAC) to ensure ongoing exploration. One could also use *noisy networks* or a decaying but non-zero learning rate to adapt to new regimes. Contextual exploration (e.g. injecting policy randomness conditioned on volatility) may help find profitable actions during regime shifts. Finally, mixing in supervised signal output into action selection (like a guided exploration bias) can be considered, but the RL policy must still discover novel strategies.

## 3. Non-Stationarity & Regime Shifts

* **Detecting regime changes:** Markets exhibit structural breaks (bull/bear cycles, volatility regimes). One can monitor indicators such as realized volatility spikes, price momentum flips, or use statistical tests (CUSUM, Bayesian change-point) to signal regime shifts. An automated approach is to include a latent *context* variable (e.g. from a Hidden Markov Model or a learned encoder) that the agent uses to adapt. For example, Chen *et al.* (2021) propose a **context-aware meta-RL** where the agent infers a latent context representation of the environment dynamics and adapts its policy safely.
* **Adaptive policies:** Two principal strategies emerge: **ensembles** and **meta-learning**.  In an ensemble approach, one trains multiple agents under different assumed regimes or time windows and switches among them based on current conditions. For instance, Trovato & Tobin (2020) train PPO, A2C, and DDPG agents on the latest data and then *re-train all three quarterly* on a growing window. They then **select** the single best-performing agent (by highest Sharpe) for the next period. This periodic retraining ensures the policy stays in sync with market changes. Similarly, one could maintain separate sub-policies for “high-vol” vs “low-vol” regimes and switch when volatility crosses a threshold.
  Meta-learning approaches train an agent to adapt quickly to new regimes. For example, Locally-Constrained Policy Optimization (LCPO) constrains updates so the agent does not catastrophically forget previous regimes. MAML-style meta-RL could also be applied, training the agent on many simulated “tasks” so it learns to fine-tune rapidly on a new regime with few samples.
* **Retraining and online learning:** Both the supervised model and RL agent should be periodically updated.  For supervised signals, one might implement online learning (e.g. fine-tuning on the latest few days of dollar-bar data). In RL, on-policy methods may be retrained from scratch or fine-tuned on rolling historical windows. The key is to avoid drift: if the supervised predictor drifts or the market features evolve, the RL agent should be re-optimized using recent data. The scheme by Trovato & Tobin is an example: **every quarter they retrain all RL agents** on the latest historical window. In summary, use sliding or growing windows to refresh both tiers, and consider ensemble/meta schemes to hedge against regime shifts.

## 4. Feature Engineering for RL

* **Trade-derived features:** In addition to raw bar data, craft features that capture execution context. Include *private variables* such as remaining inventory and time steps (as in Nevmyvaka *et al.*). Include realized P\&L or cost-to-date as state if multi-step episodes. From bar data, derive short-term momentum (e.g. last 5-bar return), realized volatility, and tick imbalance (buy vs sell volume ratio). Order-flow imbalance (net signed volume) over the last N trades or bars can hint at pressure. Also consider technical indicators from dollar bars (RSI, moving averages, etc.), but ensure they can be computed on non-time-fixed bars. These features help the RL agent learn about recent trend strength and liquidity.
* **Order-book (L2) features (future extension):** Although current L1/trade data is used, additional L2 features could boost performance. Useful candidates include: *depth imbalances* (difference between cumulative bid vs ask volumes at top N levels), *spread*, and *mid-price*. For example, classic LOB features are “total depth” (sum of volumes on both sides) and “volume imbalance” (ask volume minus bid volume). These capture liquidity and the likely short-term price pressure. Other advanced features are the *shape* of the order book (e.g. slope of volume profile) or *order-flow intensities* (rates of arrivals/cancellations). Such features inform the RL about execution risk: e.g. thin book may warn of high impact.
* **Uniquely execution-related features:** Because the agent’s goal is execution, also include features like the *current price relative to a benchmark* (e.g. deviation from VWAP), *remaining time fraction*, and *ratio of executed volume to target*. If using multi-asset or portfolio decisions, include portfolio weights or leverage.
* **Dimensionality reduction:** If using many features, an autoencoder or neural net (as in the HALOP example) can compress inputs. But even simple feature selection (e.g. PCA on high-dimensional order-book vectors) may improve learning speed.

## 5. Risk Management Integration

* **Reward shaping and constraints:** Integrate risk directly into the learning objective. One method is **reward shaping**: subtract a penalty for large drawdowns or volatility. For instance, impose a negative reward proportional to max drawdown or to large negative returns. Reward functions can also be *multi-objective*: e.g. maximize P\&L while also maximizing Sharpe or Sortino ratio. Huang *et al.* note that many RL trading studies design rewards around the Sharpe ratio to obtain risk-adjusted returns. Similarly, one can incorporate Sortino (downside volatility) or Calmar (return/drawdown) as a metric.
* **Action constraints:** Impose hard limits on actions to manage risk. For example, cap position size to a maximum fraction of portfolio, or enforce dollar-denominated risk limits.  This can be done by filtering the RL output (e.g. clipping the agent’s suggested trade size) or by including penalties for excessive Leverage. Constrained RL algorithms (e.g. using Lagrangian methods or CVaR constraints) can explicitly enforce a risk constraint.
* **Safe RL approaches:** In volatile markets, safety is critical. Techniques from safe RL (e.g. Constrained Policy Optimization or distributionally robust RL) can be adapted: for example, ensure that with high probability the loss does not exceed a threshold. The CASRL framework even uses a latent context model to satisfy safety constraints under non-stationarity. While advanced, one can simply incorporate a **CVaR-based loss** (penalizing tail losses) or use a clipped utility that grows less than linearly in profit to discourage gamble.
* **Portfolio perspective:** If Minotaur ever expands to multiple assets, treat each action as a portfolio reallocation and include cross-asset risk metrics. Even for single-asset, consider the opportunity cost of holding (interest or funding rates).

## 6. Evaluation & Backtesting

* **Simulated environment:** Build a backtesting environment using historical BTC/USDT dollar-bar data. Simulate the RL agent by feeding it sequential bars; at each step, let it submit an order and then move to the next bar, simulating fills (possibly using realistic fill models or conservative assumptions). Include realistic **transaction costs** and slippage. One should mimic the latency: dollar bars span seconds, so allow e.g. 1–5 second decision lag. Ensure no lookahead: e.g. the agent sees a bar only after it has closed.
* **Performance metrics:** Go beyond raw P\&L. Compute **risk-adjusted returns** such as Sharpe or Sortino ratio on out-of-sample periods. Track maximum drawdown, volatility of returns, and Calmar ratio. Evaluate *return consistency*: e.g. average P\&L in up-markets vs down-markets. Also measure the accuracy of Tier 1 signals (e.g. the hit rate of predicted take-profits) and how much the RL agent’s trades align with them.
* **Hybrid-layer metrics:** To assess synergy, one can define metrics like *signal exploitation gain*: how much the RL leverages Tier 1’s forecasts. For instance, measure the correlation between the predicted probability and the actual trade direction/size taken. Another metric is *information ratio improvement*: compare Sharpe of RL strategy with and without the supervised signals. One could also perform ablation: run the same RL agent ignoring the signals and measure the drop in performance. Regime-wise, measure returns in high-probability regions vs low-probability regions of the signal.
* **Out-of-sample and cross-validation:** Use strict train/validation/test splits over time (e.g. walk-forward analysis). For example, train both tiers on data up to time \$T\$, validate on \$(T,T+\Delta)\$, then test on \$(T+\Delta, T+2\Delta)\$, rolling forward. This mimics live updating. Standard statistical tests (e.g. paired t-test) can verify the significance of outperformance.

## 7. Pitfalls & Challenges

* **Overfitting (Cheating the Market):** RL agents (and deep nets in general) can overfit historical noise. This is aggravated by the sparse and autocorrelated nature of financial data. Rigorously guard against leakage: ensure the agent does not inadvertently see future information (e.g. use only past bars and features). Regularization (dropout, weight decay), simpler models, and early stopping on validation data help. Ensemble training (see Trovato & Tobin) also combats overfitting by selecting robust policies.
* **Catastrophic Forgetting:** In non-stationary environments, an agent may “forget” older regimes when trained on new data. Hamadanian *et al.* note that standard RL tends to forget past knowledge under shifting contexts. Techniques like LCPO (anchoring updates on past experiences) or replay buffers that mix old data can mitigate this.
* **Sim-to-Real Gap:** Simulated backtests (even using real historical bars) may not capture market impact or liquidity shortages. In live trading, large orders may move the market; our simulator should conservatively assume partial fills or increased slippage. Also, exchange micro-structure (order types, latencies) can differ. To narrow this gap, one might run the trained policy in paper-trading or with small live capital as a final validation.
* **Dollar-bar limitations:** Dollar bars are irregular in time, so the RL agent’s actions effectively happen at non-uniform intervals. This complicates backtesting (defining “steps” consistently) and may bias learning toward frequent or infrequent bars. Careful normalization of time or use of event-driven simulation is needed.
* **Computational bottlenecks:** Training deep RL is resource-intensive. Multi-layer CNN–Transformer networks plus episodic simulation over years of data can take hours or days on a GPU/TPU. Techniques like parallel experience collection (vectorized environments) or simplified state representations can speed this up. Memory can also be an issue if storing large experience replay buffers. In practice, one might start with smaller models (e.g. fewer Transformer layers) and scale up if performance warrants.
* **Data and regime shift:** Crypto markets can change rapidly (new participants, regulation, technology). Both tiers may need frequent retraining. Continual monitoring of prediction performance and strategy returns is essential. Alerts for model drift (e.g. supervised model accuracy falling) can trigger retraining.

## References

* Pan *et al.*, “Learn Continuously, Act Discretely: Hybrid Action-Space RL for Optimal Execution” (IJCAI 2022)
* Fu *et al.*, “Multi-Feature Supervised Reinforcement Learning for Stock Trading” (IEEE Access 2023)
* Nevmyvaka *et al.*, “Reinforcement Learning for Optimized Trade Execution” (ICML 2006)
* Trovato & Tobin, “Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy” (ICAIF 2020)
* Huang *et al.*, “A Self-Rewarding Mechanism in Deep RL for Trading” (Math 2024)
* Hamadanian *et al.*, “Online RL in Non-Stationary Context-Driven Environments” (ICLR 2025)
* Chen *et al.*, “Context-Aware Safe RL for Non-Stationary Environments” (arXiv 2021)
* Guo *et al.*, “Market Making with Deep RL from LOB Data” (arXiv 2023)
