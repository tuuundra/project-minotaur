here is the transcription excerpt from the video "The Epistemology of Deep Learning - Yann LeCun":
 Thank You Sanjeev so I'm actually going to talk a little bit about this kind of personal experience of history not because you know my experience is particularly interesting but because I think it says something about the type of thinking that people are doing in scientific ability and sometimes you know they change their mind in ways that are suboptimal so it's a little bit I was was a sort of a student of Epidemiology or interested in epidemiology in fact that's how I came to machine learning by kind of stumbling on a on a book that was the transcription of a debate between Noam Chomsky and Jean Piaget and there was an article in this in this book about the perceptron which is sort of one of the first running machines and I was fascinated by the whole thing
 that's when I was an undergrad so this the this big discussion at nips of couple years ago that Sanjay was talking about after Ali Rahimi gave his his talk where he qualified deep browning of alchemy and I took exception to this so there was a big discussion on Facebook where you know I pushed a response to his talk and then he commented on it and it's something like 200 comments or something like this so it was a very interesting discussion actually I thought but I think he's confusing two things which is alchemy and engineering now admittedly some people do engineering in ways that are indistinguishable from alchemy but you know engineering science is inventing new artifacts and there's a lot of different methodologies you can use to invent new artifacts so let's talk no let's take a few examples of artifacts like to telescope the steam engine you know things that have really kind of revolutionized the world in some way the electromagnetic in fact they are playing the you know fertilizers things like that the radio certainly and the methods for discovering those things or or creating them I mean it's a creative act it's not
 it's not sort of the analytic scientific process that that creates those artifacts it's really a creative act and every means are allowed as long as the result is you know something that's valuable and so you know you use your intuition you use your creativity you use inspiration from all kinds of sources you use tinkering systematic exploration experimentation happenstance you know a lot of things that we've discovered are were discovered by accident antibiotics for example or you know substitute sugar the best ones were discovered by a little bit by accident so all of this of course is more efficient if we have some theoretical understanding of the sort of underlying phenomenon and perhaps what what theory helps us do in the creative process is direct or creative juices and allowing us to kind of ignore the branches that are not likely to result in something fruitful so the theories that were built you know as a result of inventing those things so very often the theory comes after the invention of the artifacts the theory comes because people want to understand really what the artifact really does what its limits are how to optimize it how to make it better and there's some phenomenon that you know is discovered and and we need to understand to be able to exploit it so you know optics thermodynamics for the steam engine electro magnetics or magnetism in general aerodynamics for airplanes you know etc those things generally you know came after or the the the part of the this field that sort of explained the the artifact so it came after the artifact and the methods they are very different you know this is sort of part of of natural science now we have an artifact that we're studying and it's number of phenomena that we can observe on how this artifact
 behaves when we put in certain condition we modify in certain ways and that's what kind of builds the intuition for perhaps with the theory behind it should be so the metals there are other metals of science of natural science reproducible experiments in control conditions and of course you know the tools of mathematics mathematics statistics systematic experiments cetera and of course you know there is sort of back and forth between those two modes where one can support the other so that that's sort of you know I'm not you know telling you anything you don't know if you're practicing scientist and an engineer most of us are both and so as I said the theory often follows the invention so we can go through a few examples the telescope was invented around six you know early sixteen hundreds but theory for optics they were mostly by earlier there were theories of objects before that were completely wrong
 like for example in antiquity the theory was that it was the eye that actually produced light but you know but that was kind of putting question relatively late and the the formal theory of of optics even though lenses existed in antiquity you know putting lenses together in a complex enough way that you know we can beyond you know we can be able to stew it you know it didn't happen until then so that became of Sir prime importance to really understand optics once the telescope was invented under the microscope in various other implements steam engine actually the first steam engines go back a very long time the 1700s the first steam powered boat I think was built in the early 17th century thermodynamics didn't come until about a hundred years later that's the early thermodynamics like the Carnot cycle for example which is really the basic of thermodynamics and it was designed to really understand how you know how it steam engines work now that's really interesting because thermodynamics became the foundation of all the physics essentially that's kind of one of the most fundamental intellectual constructions of
 science and you know even though it was really developed for a particular purpose the the phenomenon electromagnetism was discovered in the early eighteen hundreds and they're the theory can came very quickly and in fact there was kind of a very quick feedback of sort of theories explained what was what was going on you know with Maxwell and and unpair and all these guys aerodynamics actually preceded the airplane but it was really designed for sailboats that's you know to kind of understand how a sail works and the notion of lifts and things like this whose Bernoulli mostly in the late 1700 but it didn't come until it was not until the invention of the airplane that the through theory of a wing if you want sort of came out which is very different formula it's not different from a sail but if you want to really have a good model of it you know the including the turbulence is that you know the wing etc it's more complicated people make compounds of course way before they really understood the underlying chemistry you know way before the atomic theory was even proposed which was only in the early 20th century and then there are things that I came more recent like the the feedback amplifier so the feedback amplifier this is a very interesting example to me if you have done any kind of electronics you know what this is
 there was a this is basically the reason why Bell Labs was created so bellas was created in the you know shortly after of World War one with one single purpose to solve they had one problem to solve which was to enable transcontinental communications at that time you could pick up your phone and call Chicago and be more or less heard from New York but you couldn't call San Francisco even if you shouted there was no there was no amplifiers right and so the the signal degraded you know over 6,000 kilometers and you know there was no signal at the other end so how do you design amplifier so of course the triode was just invented
 20s and and people try to figure out like how you you know use this for for amplification the problem with the triode is that it's a very nonlinear element and so it doesn't produce good quality amplification and one guy who's working in Bell Labs very very young Bell Labs which at the time was in head Manhattan took the ferry from New Jersey to go to to go to work and had an idea on the ferry and wrote the idea on the New York Times that he had in his hand he was reading the New York Times so there was kind of a margin and he he can I wrote the basic equation of you know a feedback amplifier in the margins you know it's basically the idea that if you have an amplifier that sort of amplify is a signal with which inverse the signal if you take a piece of the signal and feed it back to the to the input you linearize it you linearize it and every amplifier is based on this so that completely solved the problem
 so now Bell Labs had a problem because you know they were created to saw that problem they'd solved it should we just close you know close it off yeah but of course you know after that they worked on trying to make the triode smaller more reliable more efficient and then discover the properties of those kind of weird germanium crystals and now had to learn crystallography who wish they had to hire the first woman scientist at Bell Labs because crystallography at the time was kind of in the periphery of physics and so you know women at the time we're really not doing physics and when they were they were directed to areas of physics that weren't that competitive crystallography was one of them but all of a sudden they were the center of the world because that's what you needed to study semiconductors and invented transistor so so that's what this what happened so that you know that's the kind of early history of their labs and of course you know across to us there is the the computer here I'm kind of mentioning the electronic the programmable computers and the sort of electro mechanical or electronic computers 1940 well one was Connor a loser in in Germany who had kind of a relay based computer
 programmable turing-complete and then of course the ENIAC in the US but but computer science didn't become a field until the 1950s I think the the first degree in computer science was in the UK in 1953 or something and the first in in the u.s. was in 1962 so that you know it took a while before it kind of established itself as a as a science and of course you know very early on there was the telecommunication data community communication through teletypes which are basically remote you know typing machines and the information theory also coming from Bell Labs income until after World War two that was a bit of work on this but various people actually not just so Shannon before that okay so that's kind of you know the the the the magical objects of a modern world you know have been invented before they were completely understood essentially and this is a little contrary to kind of the fairy tale that we tell young students or school children at least in France where you know we have a tendency of being a little formal in terms of mathematical education where we tell them you know you do all the math understand all the theory and then only after that you can you know kind of explain you know invent things with with the theory it never works this way okay so let's talk about AI machine learning deep learning etc and the inspiration there the the creative inspiration for all of the Browning today as its roots in the 40s where with people like Warren McCulloch McCulloch and Pitt's where they were they discovered was they try to kind of draw a parallel between what a neuron does or what we knew at the time a neuron was doing and what we knew about computation
 okay and logic and what they realize is that a neuron is basically a threshold element right it computes a weighted sum of its inputs if the weighted sum is above the threshold it fires a spike if it's below it doesn't so let's think of it as a binary element right and this is actually called the macula pitts neuron so something that computes a weighted sum and turns on if it's above a threshold turns off if it's video so i sholde and what they showed is that that behaves like a logic gate and you can build circuits out of it and basically compute any logical functions you want and so you can do logical reasoning with neurons therefore that's how we think all right then
 but the same time Donal helped with psychologists had this hypothesis that in the brain you know learning takes place by modifying synaptic connections and you had the intuition that perhaps what's happening is that when two neurons were firing more or less at the same time the connection that linked them would increase in efficiency if you want in efficacy okay so that's the notion of synaptic weight which already in McCulloch and Pitt's had gonna you know hypothesized and of course this was well known from physiology physiology experiments so that's the idea of tagging running meanwhile Norbert Wiener was you know kind of wrote the book about semantics you know we we don't need the same thing today with the cyber prefix but cybernetics was a was a field that was kind of feel that concerns the the study of complex systems that are composed of modules in interaction and social feedback for example self-organization and then you know of course the optimal filters to Peleus which means you know for for living things kind of maintaining your condition or your body temperature you know things like that and number we never actually built this this little robot here which had like light sensors and this thing is it's like an insect it kind of drives itself towards the
 right okay it's very simple to build could build this you know in a weekend if you want okay yeah completely analog of course no no computers ahead and that's okay so that's the perceptron this is Frank Rosenblatt and the perceptron was really kind of the the first learning machine that was implemented with kind of an algorithm that seemed to work and do something useful really I think what Rosenblatt invented in this constant in this context is the is the concept to supervised running really of pattern recognition who supervised running and so the the perceptron was not programming a computer it was an actual analog computer where the first player was a bunch of randomly connected neurons connected to a retina which we don't see here and then the second layer was a bunch of neurons with adjustable weights and so what you see here is a row of such weights each little little thing here is one of those weights it's actually a potentiometer with a with an electric motor all right okay so when you want to train the Machine you you show an image to the photo detectors there's an array of photo cells and then you press a button for class one or another button for class two and all the motors go so how long you press basically it determines the learning rate so this concept of supervised learning is kind of what song he was presenting earlier you have you want to train a machine to distinguish images of cars from images of airplanes and your machine is basically a primary Trice function with adjustable knobs and you know you show an image of a car if the machine says car you do nothing if it doesn't say car then you tweak the knobs so that the output gets closer to the one you want okay and then you do that with image one airplane and you keep repeating this we talked some of the examples of course in a real neuron that you may have hundreds of millions of those knobs and you can view this as the process of minimizing some objective function which is the average discrepancy between the
 I'd put you want and yet what you get out of the machine okay that's supervised running so the the work on the perceptron led to kind of a standard model of pattern recognition that has survived until today but it's less prominent now in the last five years since the emergence of deep running and it's the idea by which you take a signal that you want to recognize classify whatever okay observe could be an image an audio signal the piece of text whatever then there is what's called a feature extractor which is a program generally on the computer that extracts relevant features from the from the raw signal and that module traditionally is is built by hand okay so you have several ways to build such a module by hand the stupidest way I could I could say is to just draw it randomly and that's pretty much what the perceptron was doing okay so the early experiments with the perceptron the feature extractor was basically a bunch of binary threshold units and the inputs were a small subset of pixels on the input of course the images were very low resolution and grayscale and there were maybe a few hundred of those or a few thousand of those units right so the what comes out of the feature extractor is a vector of a few hundred or two thousand numbers that represent you know the which motif is present or absent in the image and on top of this you try you you train a macular pits neuron so binary threshold unit whose weights are going to be learned through supervised running and gradient descent okay so that's a standard model of pattern recognition it's been a lot of different ways to build a classifier but the linear classifier is par by the perceptron is really the simplest one so when the perceptron first appeared there were articles in New York Times that said you know there is an electronic brain and you know within a few years maybe a couple decades we'll have you know intelligent robots and you know robot scientist and things like this right and of course this was
 you know enormous optimism and it's kind of a very common thing in the history of AI that people have been overly optimistic about the capabilities of the set of methods that they just invented perhaps a right picture is that you know you're driving at 60 miles an hour and but you're in a fog and there is you know a world somewhere but you don't see the world so you just keep driving at 60 miles an hour until you do well and that's kind of what happened so there's a famous book called perceptron written by Marvin Minsky and Seymour Papert at MIT that showed that if you have a linear classifier here and you put some constraints on what the feature extractor can do then there were certain types of classification that those systems just cannot compute or cannot compute efficiently at least and that kind of killed the whole field so what happened is is then you know funding dried for people working on this and claiming to kind of build intelligent machines so what did they do did they kill themselves in the case of Rosenblatt actually that happen he didn't commit a suicide but he fell off his sailboat and you know he was never to be found but in the case of other people working on this did you just change the name of whatever it was that they were working on they kept doing the same thing they just changed the name so they stopped pretending that those machines would eventually lead to intelligent implements they they said they called it things like adaptive filters for example okay so the Stanford team for example it was working on this and and said oh you know we could use those perceptron algorithm and similar things for adaptive echo cancellation and for like you know digital communication in fact my buses buses bus when I was hired at Bellas guy called Bob lucky invent in the low key algorithm named after him
 because and that was basically using a perceptron like learning algorithm to automatically adjust the the filter for a modem that that's what enabled digital communication through four nines okay you couldn't have that without it and so this was not you know the death of the perceptron it just took a different name and the ambition now was not to build intelligent machines which is to kind of solve engineering and that's kind of also something that you see repeating in the history of AI that whenever the ambitions of building intelligent machines dies whatever it is that people are doing kind of change his name and but it's still there like you know your GPS does path planning and that used to be part of AI not so much anymore I mean it's in every textbook but you know it's just an algorithm okay so that created a neural net winter in a sense that there was no serious researcher or very few serious researchers certainly not in the in the West working on neural nets to build intelligent machines from the late 60's to the mid 80s or early 80s I should say is that you john hopfield all right yeah so early eighties thanks to him okay
 so so people back in the in the 60s and you know already knew that they wanted to build neural nets with multiple layers they just didn't know how to train them and you know one question we can ask is why because the idea that which is using back propagation algorithm that we use in multi-layer net is very very natural it you just compute the gradient of some objective function with respect to all the parameters in your system and you take a step in a negative direction the thing is you need to have a structure where you can compute this gradient where basically it was a function input output function is differentiable with respect to the parameters and if you have binary neurons you can't do that because it you know is discontinuous so basically people people were hung up on the wrong neuron on the macular pitch binary neuron and you can't think of back propagation if you have binary neuron even though it's a completely obvious idea and in fact the idea was used in the context of optimal control for some other purpose so why were people hung up on battery neurons
 you know perhaps because they were sticking to the biology because neurons are kind of binary in the brain you know the spike or they don't spike but perhaps also because it was the limitation the hardware of the day if you were to compute electronic circuit multiplying two voltages is really complicated you need a complicated circuit for this which if one of the signals is binary is super simple it's basically just a resistor if if you write this on you know if it's a program on the computer you need a computer that can do fast floating point multiplication and you just didn't have that even when I started school in the late 70s computers were incredibly slow away from human calculation and to train you know reasonably sized neural nets at reasonable speed you need you know on the order of 100 kilo flob so maybe one mega flop you couldn't get that for less than a few million bucks
 so you know as computer scientist we'd like to think that or what we invent is not limited by the kind of hardware constraints of the computers we have at our disposal but in fact it is we were thinking is limited by the hardware that we have our disposal all the tools that we have our disposal or tools shape or imagination if you want so then there was kind of a you know a renewal of neural net and John would correct me if I said something wrong but you know it was largely because you know he wrote a paper that was extremely clear that made a connection between spin glasses which are kind of exotic crystals and kind of networks of interconnected neurons in fashion showing that maybe the the minimum the minimum of the energy of a swing glass kind of correspond perhaps to memories that you recall by kind of letting your brain fall into this minimum energy minimum there were similar work actually the to place in Japan a few years earlier that were kind of largely ignored but but it was you know a few people kind of worked on similar things but really it's a john hopfield paper that kind of revived the field and the thing is that for engineers and computer scientists neural nets were toxic
 could not mention the word neurons because you know your papers would be rejected you would be viewed as a fool because of the story of the perceptron that you know it was only 15 years earlier but but people you know still remember it vividly and so what happened is that John was Johnny's very physicist and the physicist didn't care about the taboo from the engineers and angry scientist and so the physicist basically revived the field they made the field acceptable again because they didn't have the table okay so that's kind of a mental block that stopped engineers and computer scientists from actually working on this and they had to be rescued by physicists in fact one paper that really kind of fascinated me because it was the first one that really had an algorithm that could train a neural net within units was the Boltzmann machine model by Hinton and Sinofsky just one year after John's paper in fact Terry was one of your students right yeah so it was some connection and if you read their paper it was published at triple-a I in 1983 I think and it's it's written in coded terms so never in the paper is it mentioned that there are neurons or synapses or or learning okay
 there are processing elements that are connected and they have conditional probability coefficients on them and they can do inference and the title of the paper is optimal perceptual inference okay why it's because if it had mentioned the word neurons or synapse the paper would've been rejected because it would be a neural nets again everybody knew that the perceptron didn't work so you had to kind of camouflage a little bit you know what you what you're what you're saying to get through okay then in the mid-eighties backrub just emerged people had the idea before but they never quite implemented it they never had you know they didn't have quite the right formulation those people usually by the way
 but but why did it appear only then it's because that's just around the time when you could start buying a you know a Sun workstation that had a relatively decent floating-point performance in fact the guys who did this they were my heart Jeff in turn or working on lists machines and it's the same thing it was kind of a machine that you could hurt yourself and it had so decent floating-point performance and and you know you could imagine actually running neural nets and doing multiplications and so you didn't have to you know be forced to use binary neurons so it's really the the hardware capabilities that kind of let people imagine those things which you know I find which was spectively very interesting ok but there was no clearly an inspiration from biology there in all of this and again this was a little taboo you can you can say when you're even after after then you know it was serious people wouldn't say you know we're kind of copying the brain you know because that that sounds a little presumptuous and in fact they're probably right and you know it makes you look your less serious if you clean this first of all but more perhaps arguing to or presumptuous but there is a there's a point to it which is that you don't want to copy biology too much you certainly don't want to copy biology just for the sake of copying biology without understanding the underlying principles and a good example a good good case study for this is in aviation so this is a picture of kind of an airplane from the late 19th century called a ol it was built by a guy called mo idea who was a kind of genius inventor and it was very good steam engine designer so he built this bad shaped airplane with a very efficient propeller and and steam steam engine and the things sort of managed to fly for about 50 meters at about 50 centimeters altitude okay maybe a couple flights probably destroys itself pretty quickly then you got some some money from the military he was a very secretive guy so it's kind of like oakiness right
 and built a bigger one this one this is a reproduction that the museum conservators you're not is that here in Paris if you go to Paris you should go to this museum it's great it's the Technology Museum and this one possibly took off we don't actually know because it was so secretive that it wasn't clear there's some pictures but might be fake difficult to say now what did this guy do wrong he copied bats he studied bats for a while and he just copy the shape unlike a lot of the other people I go to Lillian Charles and the and the Wright brothers and everything he did not build gliders he did not build models he did not worry about stability or controllability he just wanted to think to take off and I think took off but was not controllable I mean as you can probably imagine from the shape of it it had folding wings it's pretty amazing
 so you probably never heard of this guy clémence idea unless you're French how many French people are here okay have you guys heard of KM idea okay you have is anybody who is not french chef of kamo idea nobody okay Charlie if there are people we think their hand is because there are aviation buffs so he's forgotten outside France he left at least one legacy I mean he invented color photography and a bunch of things like this but he also left one legacy which is that this this guy is called La Veeim and Aviano is actually the word that is used to designate airplane in French Spanish and Portuguese so that's real legacy
 okay so theory is good because it makes empiricism efficient so that's the point I I made a little early on which is that you know empiricism just works you try many things and and and you see what sticks and you kind of you know build your intuition this way and sort of refine your your exploratory research that way and and sort of creative uses in fact it's something you realize when you visit Thomas Edison's works in in New Jersey in Menlo Park I think same with Facebook is but not same coast and he had shelves and shelves and shelves of every possible material you could you could you could get at that time and the reason was you get an idea you try every material to kind of try to succeed for this and you know one of them is gonna work
 perhaps he certainly did this with the lightbulb he tried many many different things and tungsten eventually worked but the problem with empiricism is that it's slow and expensive because you have to explore many thing so theory and intuition allows us to prune or empirical search space so that we can directly can I build something that has a chance for working okay so engineering neck as well as intuition and theories or guides exploration the exploration is a perfectly valid if inefficient strategy and maybe that's where alchemy resides okay
 that's why engineering has some connection with a key but your theory is really crucial it would prevent us from chasing perpetual motion for example right I mean we know we can build perpetual motion machines and the interior tells us said okay so so back to deep learning so one idea that some people had been obsessed with since the 80s was the idea that this basically feature extractor here should also be learned so instead of having a machine where you have to build this thing by hand and then train just a classifier what about a machine that can learn its own feature extractor and that's really what motivated me that's what motivated Jeff intern Terry Zaleski I mean a lot of people who kind of started working on multi-layer neural net and really the inspiration for this was reading old papers from from the 60s where people said you know it would be nice if we had a way of training multi-layer networks so how do we build a machine like this and you know the answer is pretty simple each of those machines are parametrized functions each of those blocks is a prioritized function and as long as the blocks are differentiable with respect to the parameters there I put
 differentiable with respect to the parameters and with with respect to the input then we can train this with backpropagation that's the idea of backpropagation you you can compute the gradient which is a list of numbers that indicate in which direction and by how much to change each knob for every box so that the output goes in the direction you want okay as simple as that and you can do this with a procedure that cost exactly the same as actually computing the output of this network just by running signals backwards so so then there was a little bit more inspiration from biology there was classic work in neuroscience from the late fifties early sixties by Whittle Cassie quark you know it's like Nobel Prize winning work we study the visual cortex of cats and various animals
 and figure out that there were neurons in the visual cortex that were connected to small areas of the visual field so basically locally connected and that whenever there was a neuron that was detecting a particular motif in one part of the visual field there was also another one detecting the same motif in another part of the visual field so basically what kind of replicated neurons all over the visual field so that's one type of neuron study called simple cells and then there was another type called complex cells and what those were doing we're integrating the response of the simple cells so that whenever so for example a simple cell could react to an edge let's say a vertical edge in a particular location in the image and then you displace our edge another neuron is going to turn on for that edge you displace that edge yet another neuron turns on so what you do now is make a complex cells that integrates the output of all those neurons and now that complex cell is going to turn on regardless of where the edge is within this little region so that builds a little bit of shift invariance so now if you have a whole shape and you shift that shape the representation of that shape after the complex cells doesn't change what it doesn't change much okay
 now so this gentleman kunihiko fukushima who at the time was working at the NHK lab which is the lab of the TV national TV station in japan said you know figured that he could maybe build a computer model that of kind of repeated simple size and complex cells so this is that's like a retina that's like formation a brain called natural generically nucleus and then you have simple cells complex cells simple size complex cell simple cells complex cells each simple cell is connected to kind of a small neighborhood on a previous layer and you have multiple types of simple cells detecting multiple features he did this in the late seventies early eighties but he didn't have back propagation as a learning algorithm so he designed some sort of very a dog unsupervised learning algorithm that sort of worked but it was so very difficult to make it work and his model is extremely complex it's lots and lots of knobs to turns for everything to kind of work appropriately only the last layer was trained in a supervised manner basic basically was like a perceptron but I was really kind of inspired by this work because you know it's a took advantage of the structure in images and was really inspired by Hoover and whistles work so that's what led me to be able konglish on nets which you can think of as a very simplified version of this that is trained with backpropagation on it yes and that's kind of a comes on that in action so this was you know in the late 80s early 90s there was kind of a big wave of interest in neural nets there were a lot of people working on it and a lot of activities but very few actually practical applications there were a few applications that were relatively small-scale people detecting fraud for credit card and things like this few medical applications also that were somewhat successful but no like you know a large scale application this was probably the the first one that was really our scale we even build chips for it at Bell Labs and in fact the reason so this was a group at Bell Labs called the adaptive systems research department led by larry jackel and what happened inside few years before larry jackel was promoted to department head at Bell Labs in Holmdel and is a device physicist and it was kind of trying to figure out what to do he went to talk to John and John told him well you should build electronic neural Nets the
 what he did and then he hired me I mean he hired a bunch of people like me after a couple years after two or three years and you know one of projects was building ships this is a demo of one of those character organizer this is me this is the 32 year old me I think that's actually my phone number in Holmdel at Bell Labs it doesn't work anymore so I hit a key the image is being grabbed and then recognized so this is running on a what's called a DSP digital signal processor sitting in a PC and that DSP was capable of 20 mega flops which for the time was so pretty soon we talked to a bunch of people who wanted to use this commercially and so ATM machines you know bank check readers the Postal Service for four envelopes and things like this the Postal Service actually never bought it they tested it they thought it was great but they never actually bought it
 Donnie Henderson an engineer and reach Howard I'm sure John you remember these people rich Howard was the lab director pretty soon we realized we could recognize multiple characters not just one without segmenting which was really kind of a big advance at least we thought and built what we now would call structure prediction or graphical models that's not the name they had at the time and then put this together in a giant system to recognize checks which by the end of the 1990 was reading about 10 to 20% of all checks in the US so even a commercial success the day that we invited a whole team to a restaurant to celebrate the rollout of the system AT&T announced that it was breaking itself up and the whole team was disbanded so the product group went to one of the cigar equal NCR the engineering group went to do some technologies and the research group us went to AT&T Labs and so basically the project stopped at its peak so I was kind of a big success by
 time which was 1995-96 the whole field machine-learning had lost interest in neural nets and I think the reasons for this need to be analyzed by people other than me because I'm too kind of biased perhaps but what happened afterwards you know I've worked in various places I did a short stint at a company in France and then came back to their labs and we worked on the face detection system this is very early on like before anybody in key provision could do face detection we could do this with Commission that's really fast so you know the second year on that winter occurred despite the fact that those systems actually worked and we have to ask the question why
 so certainly hardware was slow for upon calculation as I was saying even you know CERN or sequin graphics workstations and so we could only train very small companies by today's standard but you know they were still they were good for character recognition perhaps also speech recognition although they were never really superior to other techniques data was scarce and neural nets for data hungry and so there was no large data sets besides character and speech recognition it was very very expensive and difficult to collect data back then and so there were only a few things you know it was no internet right so how do you collect data when you don't have the internet and so so that limited the scope of applications of those things if you wanted to build your you know deep learning software framework to train those systems experiment with it
 you know display you know figure out debug you know all that you had to build everything from scratch like you know there was no Python there was no MATLAB there was no things like that you have to write your own programming language basically okay which is what we did in fact that's probably one of the reasons why we were for a long time the only people who could train commercial Nets because we are the only people who spent like a year or two developing or complete deep learning environment all of the modern deep running frameworks that people use today are basically you know descendants of this one that little bit you and I wrote and then open sourcing was not common in the pre-internet days if you work for a company there was no way you could pass the lawyer's to release your code
 and so what happened now there was is that there was a whole bunch of tricks that were implemented in those you know the the alchemy the black art of neural nets were implemented in the you know code but we could not release it and then putting all of this in the wrong paper sort of didn't make much sense like who cares but the tools gave her superpowers okay now what happened is about a year after I joined Bell Labs so I don't realize in 1988 and about a year after in 1989 a group hired vladimir vapnik the pekin here
 vladimir vapnik was a russian mathematician he has close friends here in the room and he lives not very far from here actually and he had developed this this whole theory which he continued when he after he joined Bell Labs that was basically a very foundational theory of running in general not just for machines just running so in particular it you know we produce one of the you know a bunch of those formulas that says if you train a machine on a on a training set and then you have a different set a test set where the samples are drawn from the same distribution as the training set then you can show that for a certain class of machine that is that doesn't have sort of infinite power as you increase the number of training samples the error on training and the urine tests are going to converge to towards each other so with infinite data the the errors you get on the training set on tests that are the same
 the small amount of data the error on the test set could be higher than on the training set for a given amount of data if you change the capacity of the Machine the size of the neural net if you won the number of parameters is something like this you're going to have a curve like this which Assange Eve actually showed where you know there's an optimal size for your neural net below which you get a lot of errors above which you also get a lot of errors and it's an optimal size where there is a gap between training and test so this was conceptually magnificent there was all kinds of formulas and people try to use this for practical things like determining what size should I give to my neural net or what
 you how many parameters you know can I predict the generalization error on the test set that my machine will do and that basically never worked so it's conceptually incredibly illuminating in practice basically zero impact so what people do to select a model is the do validation of cross-validation they train on a particular set and they measure the error on another set and those formulas basically are completely useless for that purpose there's been lots of attempts lots of papers but nobody does it in practice and it's partly because those those generalization predictions are generally widely over generally why do you overestimate the generalization error and so in practice is much better than in theory and then there are the problem that sanjeev mentioned which is that you know systems we trained today may have hundreds of millions of parameters we only turn them with a few million training samples all the statistics every textbook in statistics we tell us this cannot possibly work it will be over
 parametrized and your error and the different set will be crazy but it works and it's one of the mysteries one of the theoretical mysteries of deep longing neural nets but it's not because we don't understand this from the theoretical point of view that it's not true empirically we've known that it's true empirically for decades but somehow people were still trying to convince us that this could not be this Newton could not be true it was clearly true so that led to actually a bet because you know within the same within two of his separation in the same aisle there was you know me and number two and version of all the people working on their own net and then down the hall
 venya vapnik he's a big you know Konya cortez bonobos are working on super vector machines kernel machines and this was the department head every jackal and you know he he made a bet with vat nick and the bet is as follows jackal bets one fancy dinner that by march 14 2000 people will understand qualitatively why big neural networking on our databases are not so bad okay that was a mystery why is it that they work so well even though the theory is not understood I think that's one fancy dinner that jackal is wrong but if vapnik figures are the balance and condition that mixture wins the bed okay this was a thinly veiled incentive designed by Larry to convince that peak that you should work on the theory of neural nets he never did and so they they signed I was the witness and this was you know in 2000 but in 2000 the jury was still out so we waited until 2005 and in 2005 was clear that neural nets were coming back and so but there was no theory so they basically both lost or they both won the both lost actually so the invited Leo and me to dinner and they shared the check and the own I just enjoyed the dinner so after the the second neural net winter there was a spring and you know the reasons for why is it that the Michigan cavity kind of turned away from neural nets despite the empirical evidence so they were working and I think it's a it's a case of mathematical heap losses and you know it's it's amazing to think that it actually exists and like it could be sort of collective delusion that something doesn't work because we don't understand it understand it theoretically but it is true that's at least the best explanation I can come up with so what you what you get with super virtual machines and and and other methods are simpler if you want a neural net is you get you know better theory well you can prove stuff you get you know generalization bounds which is cute you get a convex optimization which is has portable convergence you're not using stochastic optimization because the math is too hairy you know there is all these reproduce in kernel Hilbert space kind of formalism that explains what those support vector machines are doing is very cute mathematics you can put you know drop names in your paper like primary dual matters care wouldn't occur condition and
 you know civility programming and that gets you know that impresses the reviewers you know and you know other methods were invented around the same time variational based methods and nonparametric Bayes and you know various types of graphical models and conditional random fields but well but so we had back pops we had Cornett with structure prediction the community insisted on moving towards SVM Gaussian processes here - nonparametric Bayes none of these methods ever be components on standard data sets like a mist which is you know how many ten digits ever but somehow they there was a legend that they did it's amazing I had to like make a webpage with a big table that showed no that's not the case
 even then so I think ok I I kind of pulled this off of ok there was two years ago at nips there was the the rigor police which we know been wrecked and and a bunch of other people Shelly Schwartz I think that is very poor yeah that's right brilliant people okay and they kind of you know label themselves the rigor police and you know it's good to have mathematical rigor but if you have that in excess it leads to excessive rigor so I I you know I try to figure out like what's the phenomenon that causes people to kind of move away from it empirically successful methods just because they can explain the other ones and it's a bit like the old joke right that the it was a streetlight effect and and you know Noam Chomsky no I don't agree with him when he talks about coaching psychology and linguistics but but he has a point here science is a bit like a joke about the drunk who is looking under the lamppost for a key that he has lost on the other side of the street because that's where the light is it has no other choice and so it's a bit of this phenomenon that I
 think we we observe there that you know people work on stuff that they can understand as opposed to stuff that actually works and then work towards understanding those things that actually work so you know a bunch of lessons that we learned here Hardware mutations influences research directions good software tools shape research and gives us super power hardware performance matters you know the recent revolution or emergence of deep learning actually is is evidence to this and here is something weird when hardware is too slow software is not really available or experiments are not easy reproducible because of lack of open source people will find ways to dismiss and abandon good ideas so I was hearing things in the late 90s early 2000 like yeah you know youngin can try and congressional nets but he's you know the only guy in the world who can do this like nobody else can do that
 you know today there's probably millions of people who can do it so it was just not true it was kind of a way of you know can i dismissing the the fact that you had to kind of do the effort of implementing things so I became a big advocate of open-source after that and you know despite all the acute mathematics so here is an example of mathematical hypnosis which is the fact that when you look at what a kernel machine or super vector machine actually does you can write 500 page books on the mathematics behind you know underlying it which is beautiful you know which employs all the keywords that I mentioned earlier you know double spaces you know spaces of infinite dimensions and reproducing kernel info spaces there's all kinds of really good mathematics you can do for those things but in the end what they do computationally is incredibly naive what a super virtual machine or combination does is it takes an input vector it compares this input vector to every training sample that you have basically you have it's like a two layer neural net where the first layer has as many neurons as you have training samples and what this
 does is that he compares input to every training sample okay and he passes through a function a kernel function that produces a score some kind a number and then the classifier just computes a weighted sum of those things okay it's a two layer Network where the first layer is not learned or at least is not run supervised it's just a copy of the training samples computationally that's incredibly naive I would say there's a lot of functions you cannot represent efficiently this way but when you talk to a theorist you tell you look you know this theorem I can approximate any function I want on my training set as cool as I want why do you need anything else you know it took 15 years to convince people that it actually helps to have multiple layers okay I'm at a time
 so I will thank you very much for for your attention and take questions if we attack ha well I mean essentially can I hit all the right buttons there is we don't understand why they generalize despite the fact that they are you know way over parametrized there's quite a bit of work on this which is revolves around the idea that somehow the intrinsic dimension that actually matters that learning actually works in is much lower than the number of parameters most of the parameters get wasted they don't get completely wasted because they are there to facilitate optimization so if you have a neural net that you want to Train for a particular problem and somehow you figure out the optimal size of that neural net just the right size so that you can just learn the training set it's going to be very very very hard to train it if you make that neural net five ten times bigger than necessary then it's much easier to train why it's because you know in high dimension if you have local minima or etc you know you can go around mountains you basically have the generate dimension of the solution space which you know is very high and there's a number of papers on this and perhaps a new resource I was talking to someone here from AI yes so it's quite a bit of work on sort of trying to estimate what is the dimensionality of the solution space as a function of the number of training samples the complexity of the task and the size of the neural net so what we're doing wrong back in the 80s is or neural nets were too small there were just the right size for the problem and half the time doesn't converge so a lot of people would try this make a few other mistakes and and include that neural nets didn't work they were just not doing it right and they couldn't rely on open source code that you couldn't just get from other people so that's one question
 okay generalization second question is how is it that when you make what's the shape of the landscape that is such that when you make the neural nets because unnecessary optimization becomes easy it's very reliable right I mean you you train your on that you always get solutions that are similar in performance it's never the same solution because it will fall in different part of the space but it's you know quite reliable so those are like two really big questions I would think and then there are kind of more specific questions of which are more kind of practical you know how do you accelerate learning with you know various atomization tricks are you you need to like normalize the way it's are you train recurrent Nets which have kind of their own type of problems I mean there's a bunch of different things like this but but those are I think the two big little big questions yes I can get to this because yeah I have a whole section or we first met Ronnie but so interesting you know if the history I told here there is a parallel history in reinforcement learning I think she would slap me if I if I say wrong correct me if I if I say just now but there were some really interesting work in France went running in the early 90s by among others Jarrett Azzaro who used a neural net to compute the value function the evaluation function of a backgammon player so this was one of the first really successful examples non-trivial examples using refrance meant running to get a machine to play a complex game and you know machine actually worked really well he could be beat the world champion at some point so that was really amazing by the time he had that success which was about the same time that we were producing those character recognizers the reinforcement running community completely lost interest in your own ads also and started working with much simpler models where the decision function or the policy is basically a table with probabilities so there's no privatize function is no neural nets and because it's easier to study theoretically and then they prove that you know these conversions proofs when when it's you know you don't have those tables you know those known that you don't need have tables you know I'd known that and then you took 15 years again for reinforcement learning basically to decide to use neural Nets again which no had happen
 great I think the part where I would add a bit of nuance is your earlier comment where you said that when you run a neural net the results you get though it may not be the same solution the quality of the results is essentially comparable that's right and what we saw in reinforcement learning in fact is you run the neural net a few times when you reinitialize the weights you get a lot of variability in the performance of the system and it's that high variability that scared some people away from using neural network for reinforcement learning and in the last few years five years or so we've had much better understanding which didn't necessarily come from theory but we've had much better understanding of what kinds of mechanisms are necessary to stabilize that variance and give us more consistent empirical results right I mean I think you know it's I mean I'm sure you're going to talk about this in your talk so and you are the expert on this but there is oh let's take a state City there in reinforcement learning that is the fact that the observation you're going to make it at time T depends on the action you took at it at the previous time so then it releases a lot of noise and stochasticity in the system and probably some problems that you know partially explains how the department's are observing but you know for RL which don't seem to occur for supervised running so I believe during your talk you said something along the lines of following biology too closely has not led to desired results yeah
 so today's field seems to be all about bigger and bigger data sets and more and more processing power however that's achieved and I'm just wondering though you know are we operating in a way that's sort of orthogonal to the way the brain works in that I mean we all know we've got you know we have all have these 20 watt processors that don't necessarily process very quickly and yet we also all know that we don't have to show an infant millions of training samples to tell the difference between a dog and a cat much less more complex great thing so I just wondering are we going down the wrong path or is this the only thing we can do because it's all we can really understand so that's my other talk ok I so my standard talk is this is not much time to talk this is more kind of historical philosophical thing which I apologize for but my boys time to talk is exactly that point that we're not going to build two intelligent machines using either supervisor running or pure reinforcement running we're gonna have to use those things as well but they are actually a relatively minor part of overall learning in my opinion and the type of learning that humans and animals do is mostly I would call it self supervise which means that we learn how the world works by observation largely also a little bit by action but mostly by observation like babies are basically helpless in terms of action but they're on an enormous amount of you know background knowledge about the world just by you know watching we learn intuitive physics we learn you know gravity you know it takes about seven months for that we learn object permanence all kinds of stuff now how do we get machines to learn this way just by observing not by trying to solve a task supervise the Badgers watching videos for example that's the big question there's a big challenge in AI for the next few years and it's really promising approaches there so things like generative a virtual networks and other types of latent variable models that basically deal with the fact that those predictions that we you know one example of our u-20 machine like this is you show it a few friends over video and you ask you to predict what's gonna happen next and if you can do this relatively reliably with you know some decent future that means it probably has understood what it means to you know that the world is really mentioned all the objects they can move in the penalty of other is that you know they have to obey physics somehow that there are animate objects an inanimate object is a lot of stuff that the machine will have to learn if it's going to be able to make those predictions so so that's so that approach of cosmic rezoning works extremely well in natural language processing over the last year or two has been I mean also starting before this we were starting with the Columbia Western paper from the 2009/2010 where they can learn word embeddings by basically just training a neural net to kind of predict other words then there was you know war to vac which is a very popular technique very simple to learn vectors associated with words in a language so we want to represent the
 language also by using prediction you take a bunch of words and you train a machine to predict the word that's missing in the middle and now there's those new techniques like Bert and transformer networks you know with B - things like this where you take a segment of text could be a dozen words from a corpus and you train a giant neural net to predict words that have taken out you take out 15% of the words from the sentence and you ask the neuron that predict what words are there that works really well for text because it's easy to represent uncertainty in text it's just a long list of numbers between 0 and 1 that's 1 - 1 we don't know how to do this with friends over video because we don't know how to represent interesting probability distributions in high dimensional continuous spaces that's a challenge but we'll find solution eventually the big question is are we going to find a solution before the people funding or research get get impatient no there are many interesting questions but yawn will be here also for the panel discussion and I hope he'll answer more questions then so let's talk
 and the time constraint is that lunch is at the dining hall across the greens over there and just follow the crowd and we want to be back let's say we run five minutes late five past two for joyless talks so there's not a lot of time for all the people to go through the dining hall at so let's be efficient thank you