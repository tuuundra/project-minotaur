Based on the source provided, here is the transcription of the excerpts from the video "[MXDL-11-07] Attention Networks [7/7] - Stock price prediction using a Transformer model" uploaded on the YouTube channel "meanxai":
 This is the final part of a series on Attention Networks. Before I end this series, I would like to write some code to predict stock prices using the Transformer model. This video was produced in Korean and translated into English. My voice is generated by AI, Text-To-Speech. Since stock prices are also time series, we can apply all the Se2Seq, Attention, and Transformer models we have looked at in this series to try to predict stock prices. In this video, we will use the Transformer model to predict stock prices.
 Stock prices are difficult to be predicted because they are characterized by non-stationary stochastic processes, that is, random walks. It can be said that future stock prices are determined not only by past memories but also by future events, information shock, etc.. Past memories can be technically analyzed, but future events cannot. Therefore, predicting future prices seems impossible. The transformer model can only learn from the past memories that the stock chart has. Nonetheless, to learn how the Transformer model works, let's apply it to stock price prediction problem.
 Please note that this experiment is not intended for stock investment but to familiarize you with the Transformer model. For a more stable prediction, let's predict the normalized stock price with the trend removed, rather than the raw stock price. Here are the historical charts of the S&P500, DOW, and NASDAQ over the past 25 years. You can see that the stock prices fluctuate with their long-term uptrends. Typically, stock prices are non-stationary time series. That is, stock prices are a time series with a trend, and their statistical properties change over time.
 These properties can make it difficult to predict non-stationary time series. For this reason, we would like to predict the weak stationary time series where the long-term trend is removed. Removing the short-term trend would produce a strong stationary time series, but this is not easy, so we will only remove the long-term trend. Here are the stock charts with the long-term trend removed. And standardize the stock prices. Our goal is to forecast this time series using a Transformer model. We can obtain the predictions for the original time series by transforming this results back to the original scale and reinserting the long-term trend.
 Now let's write a code to preprocess the stock prices data. First, let's get historical stock data for the S&P500, DOW, and NASDAQ from Yahoo Finance. This data includes the open, high, low, close, and adjusted close prices. We will only use the adjusted close price. The number of data points is 6225. Next, let's remove the long-term trend from each price. There are many ways to find trends. The long-term trends can be found using linear or nonlinear regression, or they can be derived statistically. Here we will use simple linear regression.
 Find the linear regression line for each price. These are the long-term trend lines shown in the first chart on the previous page. Next, we remove the long-term trend from each stock price. This is the second chart on the previous page. Next, we normalize each stock price. Then we split the normalized data into the training and test data. The last 20 data points are used as test data and the rest are used as training data. Finally, we use the training data to generate a dataset for the Transformer's encoder and decoder.
 For more information on how to generate this dataset, please watch the first video in this series. Save the preprocessed dataset to this file. Let's run this code. The results are as follows. Next, we will write code to train the Transformer's encoder and decoder. This code is identical to that in the previous video. Only the model's arguments have changed. The sequence length of this dataset is 60. The Transformer will learn the patterns for the past 60 trading days. 60 trading days correspond to approximately 3 months.
 Features are S&P500, DOW, and NASDAQ. The number of features is 3. And d_model is set to 120. The number of layers in the encoder is set to 2. The number of heads in the multi-head attention is set to 4. The number of hidden neurons in the Position-wise Feed Forward Networks is set to 128. And to prevent overfitting, we set the dropout rate to 0.5. The arguments of the decoder are also set the same as those of the encoder. Create and compile the transformer model. And fit this model to the training data.
 Once training is complete, save the parameters of this model to this file. You can see that the training loss is significantly reduced. Let's run this code. The training is complete. The trained parameters of this model are stored in this file. Next, let's write code to predict stock prices using the trained Transformer model. This code is also identical to that in the previous video. Read the training data and test data. Create an encoder and decoder, the same as on the previous page. Create a transformer model and load the trained parameters.
 The historical time series displayed in the results are the last 50 data points of the training data. Predict the stock prices for the next 20 trading days. 20 trading days correspond to approximately one month. As we did in the previous video, we feed the last 60 data points in the training data into the encoder. We feed the last data point in the training data into the decoder. Then, we repeat this 20 times, feeding the output of the decoder back into the next sequence of the decoder one by one. Once the iteration is complete, you will get the stock prices for the next 20 trading days.
 For a detailed explanation of this process, please watch the previous video. Visualize the last 50 data points of the training data, test data, and prediction results. The results are as follows. The left chart shows the time series of the last 50 periods of the training data, and the right chart shows the actual time series of the test data. And these solid lines are the predicted time series. As mentioned earlier, the Transformer can only learn from past memories and cannot know future events, so the predicted results do not match the actual results.
 Remember, this is a chart with the long-term trend removed. To obtain the predictions for the original stock prices, we need to reinsert the long-term trend into the predicted time series. We will skip this step. Let's run this code. The results are as follows. Finally, let's observe the results when we slightly change the configuration of the Transformer model. When I set the Transformer arguments like this, I got the following results. The first run gave these results, and the second run gave these results.
 And when I set them like this, I got the following results. The first run gave these results, and the second run gave these results. So far, we have looked at all the attention networks through seven videos.
