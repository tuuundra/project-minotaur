Great. I will conduct an in-depth investigation into the optimal method for representing tick-by-tick BTC/USDT trade data for use in a CNN-Transformer-based high-frequency trading model. This includes a rigorous analysis of aggregated bar techniques (time-based and event-driven), direct tick sequence modeling, and hybrid approaches. I will evaluate these methods through the lenses of information theory, model architecture suitability, empirical performance, and practical constraints specific to cryptocurrency markets.

I will include comparative tables, technical analysis, and cite both academic literature and high-quality industry insights. I’ll let you know once the report is ready for your review.


# Representing BTC/USDT Tick Data for a CNN-Transformer in HFT

## 1. Theoretical Foundations and Information Loss in Data Aggregation

**Information Loss in Bar Aggregation:** Financial tick data (each individual trade) carry high-resolution information, but are noisy and irregular in time. Aggregating ticks into bars – whether time-based (e.g. 1-minute OHLCV) or event-based (e.g. every *N* trades or *X* volume) – inherently filters out details. By compressing many trades into a single bar, we reduce noise and data size at the cost of discarding the sequence and timing of trades within that interval. For example, a candlestick’s thin wicks might imply little activity occurred at the extremes, yet sometimes most price action actually transpired in those tails – a nuance lost in the OHLC summary. From an information-theoretic perspective, regular time sampling can be viewed as *uniform downsampling* of an irregular event-driven signal, which risks aliasing and loss of entropy: important microstructure events may be averaged out, and the temporal ordering of trades within each bar is not captured. In essence, time bars act as a low-pass filter on the tick sequence, smoothing out high-frequency fluctuations (reducing noise) but also potentially obscuring short-lived predictive patterns (losing signal).

**Market Activity and Sampling Bias:** Markets do not process information at a constant rate, yet time bars sample at constant intervals. This mismatch causes information loss and noise. During high-activity periods, many impactful trades occur but a time bar will condense them into one interval, hiding intraperiod dynamics; conversely, in lulls, time bars produce data points even if little happens. This introduces *sampling error*: the same number of bars are produced in quiet noon hours as during volatile news spikes. As a result, time bars “disguise the rate of actual activity” and introduce extraneous noise for models. Event-based bars aim to fix this by synchronizing sampling with market activity. Tick bars (every *N* trades) or volume/dollar bars (every *N* contracts or *\$X* traded) allocate more bars when trading is intense and fewer when activity is low. Thus, each bar represents a more consistent “information packet” of market activity, mitigating the bias of clock time. In *information-theoretic terms*, event-based sampling attempts to equalize the information content per sample, thereby preserving entropy that time bars might unevenly squander.

**Statistical Properties – Stationarity and Distribution:** The choice of sampling affects the statistical nature of the time series. Time-bar returns often exhibit significant **serial correlation, heteroskedasticity, and non-normal distributions**. This is partly due to volatility clustering: periods of high volatility produce sequences of large returns within short time, followed by quiet periods – an artifact amplified by uniform time sampling. Event-driven bars can reduce these effects. Notably, tick returns (returns sampled every *N* trades) tend to behave “much more like i.i.d. Gaussian” variables. Empirical tests confirm that volume and dollar bars yield return distributions closer to normal (lower kurtosis) and with more stable variance than time bars. **Figure 1** below illustrates how event-based bars recover normality: the return distribution of time bars has a sharp peak and fat tails (red line), whereas tick, volume, and dollar bars progressively flatten the peak, approaching the bell-curve of a normal distribution.

&#x20;*Figure 1: Distribution of returns for different bar types (Time, Tick, Volume, Dollar) compared to a normal distribution. Time-bar returns are sharply peaked with heavy tails (red), indicating strong non-normality. Event-based bars (tick, volume, dollar) show returns more spread out and closer to the dashed normal curve, evidencing a partial recovery of normality.*

Furthermore, event-driven sampling can alleviate **autocorrelation** in returns. Each time bar often contains an unequal number of information updates, leading to artifact correlations between adjacent bar returns. Volume and dollar bars, by equalizing traded volume per bar, produce returns with markedly lower auto-correlation. **Figure 2** demonstrates that autocorrelation of returns is highest for time bars and progressively diminishes for tick, volume, and dollar bars. In short, *time bars induce serial dependence and heteroskedastic variance*, whereas event-based bars yield more stationary series – a desirable property for many modeling techniques.

&#x20;*Figure 2: Autocorrelation (ACF) of bar return series (first 10 lags) for Time bars vs. Tick, Volume, and Dollar bars. Time bars show significant positive autocorrelation at lag 1 (and beyond), indicating lingering dependencies. Tick, volume, and dollar bars have autocorrelations much closer to zero at all lags, with dollar bars showing the weakest autocorrelation. This suggests event-based bars better approximate an i.i.d. return process.*

**Trade-offs – Noise Reduction vs. Signal Degradation:** By aggregating ticks, we reduce microstructure noise such as bid-ask bounce or random trade orderings. This can enhance signal-to-noise by smoothing out ultra-high-frequency jitter. Many technical indicators and machine learning models implicitly assume a degree of smoothness or stationarity that raw ticks violate. Bars provide this at the cost of temporal precision. The key question is whether the removed “noise” might contain exploitable signal. Some informational patterns – e.g. specific order flow bursts or sequences of trades – are lost when looking only at OHLCV of a minute. For example, a rapid series of buy ticks lifting the ask (a bullish order flow imbalance) might be predictive, but if they occur within one 1-min bar, the bar’s net change could be small, hiding that brief momentum. Thus, aggregation can **filter out short-lived predictive micro-patterns**, a form of *signal attenuation*. From an information theory lens, one might view each tick as carrying bits of information (some signal, much noise). Time bars compress those into a few summary statistics, potentially discarding bits that could improve predictability. The optimal aggregation level thus represents a balance: coarse enough to remove pure noise and make learning feasible, but fine enough to retain critical market microstructure signals. In practice, the superiority of one representation depends on the efficiency of markets at those frequencies – if most tick-level fluctuations are indeed noise, bars will help; if they contain alpha (e.g. informed trading patterns), over-aggregation will hurt model performance.

Finally, *information-driven bars* (imbalance bars) take this idea further: they do not just sample every fixed volume or trades, but trigger a new bar when a statistically significant *order flow imbalance* occurs. The idea, introduced by Lopez de Prado (2018), is to **sample more frequently when new information arrives** – effectively using the data’s own irregularities to decide sampling. Imbalance bars attempt to preserve even more information by catching shifts in buy/sell pressure in real-time. These advanced bars theoretically minimize information loss by aligning sampling with the arrival of informed trading, thus **maximizing entropy per bar**. They further reduce noise by not sampling during equilibrium, only when imbalance deviates beyond expected thresholds. In summary, as we move from time bars to tick/volume bars to imbalance bars to raw ticks, we increase information content and irregularity, while moving in the opposite direction increases regularity and noise-filtering at the cost of information. Understanding this trade-off is crucial before feeding data to a deep learning model.

## 2. Empirical Evidence and State-of-the-Art Comparisons

**Research on Different Input Representations:** A growing body of research in algorithmic trading and financial machine learning has empirically compared model performance using different data representations. A consistent finding in both academic studies and industry white papers is that **using tick data with event-based bars can improve model learning and predictive power** relative to traditional time bars. In their open-source project, the Hudson & Thames team demonstrated that converting raw tick data into volume, dollar, or tick bars yields **better statistical properties (more normal, less autocorrelated returns)**, which “in turn helps machine learning algorithms learn and predict”. They showed that a simple random forest model trained on event-bar features outperformed one trained on time-bar data, highlighting that the choice of sampling can materially affect model accuracy. This aligns with earlier findings by Easley *et al.* (2012) that *volume-synchronized sampling* better reflects information flow and can improve prediction of order flow toxicity.

In high-frequency equity trading, studies have benchmarked deep learning models on different input types. For example, Zhang *et al.* (2019) in the DeepLOB model implicitly leveraged high-resolution data by using sequences of order book snapshots (which are effectively tick-by-tick state data) and achieved state-of-the-art accuracy in mid-price prediction. While DeepLOB did not explicitly compare against lower-frequency bars in that paper, its superior performance over older models suggests that feeding the *fine-grained limit order data* (a form of tick-level information) enabled the CNN-LSTM to detect patterns missed by models using coarse data. Subsequent research has made more direct comparisons. Sirignano and Cont (2019) found that neural networks can indeed learn from order-by-order data of millions of trades, capturing subtle microstructure patterns that disappear with aggregation. On the other hand, a recent study by Kang *et al.* (2024) on Korean stock data showed that an LSTM on raw OHLCV bars (daily) could match the performance of models with extensive hand-crafted features. This suggests that at lower frequencies, the model’s own feature extraction can rival traditional feature engineering. However, at *higher frequencies*, the consensus is that **using more informative sampling (or raw tick sequences) provides an edge** – provided the model is capable of handling the complexity.

**Performance in Crypto vs Traditional Markets:** Within cryptocurrency markets, a few studies have addressed the impact of input resolution. Most academic crypto prediction works still use minute-hourly bars, but some practitioner analyses and competition results indicate that **models ingesting higher-frequency data can outperform** those using only coarse data, especially for short-term forecasts. For instance, one Kaggle-style experiment (Omole & Enke 2024) compared a CNN–LSTM on minute-level data to various architectures, finding that the CNN–LSTM (which can capture both local patterns via CNN and longer dependencies via LSTM) gave the best accuracy for Bitcoin price direction. While that study didn’t explicitly test tick data, it underscores that *hybrid deep models thrive on richer data*.

A more directly relevant work is by Woojin Kim *et al.* (2023, *Expert Systems with Applications*), who proposed an attention-based CNN–LSTM specifically for high-frequency cryptocurrency trend prediction. They introduced a novel labeling method and compared multiple frequencies. Their **attention-CNN-LSTM model for multiple cryptocurrencies (ACLMC)** was designed to exploit multi-frequency data simultaneously, effectively a hybrid approach. It outperformed models that only used single-frequency bars, illustrating that incorporating tick-level fluctuations alongside, say, 1-minute bars boosted predictive performance (especially for capturing quick pump-and-dump patterns in crypto). Similarly, *HelforMer* (2023) and other attention-based crypto models have begun leveraging finer data, although robust benchmarks are still developing.

On the other hand, there is evidence that not all tick data usage guarantees improvement. Easley *et al.* (2012) noted that inferring buy/sell volume from raw tick-by-tick might be “inefficient and costly” without much accuracy gain. This was context-specific (their VPIN metric), but it warns that **naively modeling raw ticks can be futile if the model doesn’t effectively extract signal**. The deep learning literature addresses this by developing architectures to handle noise (see Section 3). When done properly, tick-level modeling often yields state-of-the-art results. For example, Lai *et al.* (2023) introduced a *Differential Transformer Neural Network (DTNN)* for high-frequency stock data which combined a temporal CNN and a Transformer – this model, fed with full LOB tick data, outperformed other methods on a standard LOB benchmark (FI-2010). The authors emphasize that extracting effective information from LOBs (which are noisy, high-dimensional tick data) was key to beating the state of the art. In summary, the **state-of-the-art HFT models increasingly use raw or minimally aggregated sequences** – but crucially, they pair them with architectures explicitly designed to tame the noise and complexity (e.g. using convolutions, attention, and specialized layers).

**Benchmarks and Best Practices:** There isn’t yet a universal benchmark dataset comparing all representations on the same footing (especially in crypto). However, best practices have emerged:

* **Use event-based bars over time bars** for initial feature extraction. Multiple sources show dollar or volume bars yield more stationary features and often improve model training stability. For example, a crypto trading firm report found that a simple momentum strategy on volume bars outperformed one on 1-minute bars in backtests, due to the reduction in false signals during low-liquidity periods.

* **Incorporate tick data for short-term signals:** If computationally feasible, include tick-by-tick data or indicators derived from them (order flow imbalance, volatility bursts). Competitions and proprietary research often find that adding microstructure features (like queue imbalance or trade flows) improves short-horizon predictions of price moves.

* **Hybrid or multi-scale models (see Section 6)** are increasingly viewed as best practice: They allow the model to get the “best of both worlds” – noise-filtered context from higher-level bars plus raw detail from tick sequences. For instance, one could feed a Transformer with a sequence of imbalance bar returns (stationary, lower frequency) alongside another input of recent raw tick changes, letting the model attend to both long-term and immediate patterns.

In conclusion, empirical evidence strongly suggests that **finer data representations can enhance deep learning performance in HFT**, but only if the model is sophisticated enough to digest them. When model complexity or data quality is limiting, a well-chosen aggregation (like dollar bars) may be the optimal input. The literature encourages experimentation: for a given problem, one should try multiple representations – time bars, various event bars, and raw tick sequences – and evaluate predictive performance, as *“one type of bar is not necessarily better than another” and the optimal choice is data- and model-dependent.*

## 3. Model Architecture Implications of Different Data Representations

**CNN-Transformer Architecture Overview:** In the context of time series, a *CNN–Transformer* hybrid model typically uses convolutional neural network layers to extract local patterns or features, then a Transformer (with self-attention) layer to capture longer-range temporal dependencies and relationships among those features. This architecture is well-suited to high-frequency trading data if designed properly: CNNs excel at handling the locality and noise in rapid tick fluctuations, while Transformers can model the sequence’s global context (e.g. regime shifts, trend reversals). The input representation – whether minute bars or raw ticks – significantly influences how we design and parameterize these components:

* **Length and Irregularity of Sequences:** Raw tick sequences are far longer and irregularly spaced in time compared to, say, 1-minute bar sequences. A CNN-Transformer model ingesting raw ticks might need to handle thousands of events per hour, versus 60 data points per hour for 1-min bars. This has architectural implications: the *positional encoding* or time embeddings in the Transformer must account for irregular time gaps between ticks. One strategy is **time decay encoding**, where we encode the actual timestamp or the inter-arrival time as a feature fed into the model (so that attention can consider how far apart in time two ticks are). Alternatively, one could resample ticks to fixed intervals (introducing null ticks or padding), but this reverts to a time-bar approach and could reintroduce noise in quiet periods. Recent research on irregular time-series suggests using *continuous time positional encodings* or modifying self-attention to weight events by time difference.

* **Convolutional Kernel and Feature Design:** With different inputs, CNN kernel sizes and feature maps must adapt. For **time-bar data**, each data point might be a vector of features (OHLCV or technical indicators) representing a whole period. Patterns of interest (like a particular candlestick formation or multi-bar pattern) may span, say, 3–10 bars. Thus, CNN kernels could be wider (covering multiple time steps) to capture these patterns. In contrast, **tick-level data** often require detecting very short-term patterns – e.g. a sequence of 5 rapid trades that push the price up. CNN kernels for tick sequences can be kept small (e.g. 2–5 events) focusing on microstructure motifs such as price upticks with rising volume. The CNN must also handle *multi-dimensional input*: a tick might be represented by price change, trade size, order book imbalance, etc., and one can imagine the tick sequence as a multivariate time series with those features. Some HFT models (e.g. Zhang *et al.* 2019) treat an LOB snapshot as an “image” and apply 2D CNNs, but for trade ticks a 1D CNN suffices, possibly with multiple input channels (price, volume, etc.).

Importantly, the model’s first layers often need to **denoise or normalize** irregular tick data. Lai *et al.* (2023) introduced a *Temporal Convolutional Network (TCN)* as part of their feature extractor to *dilate* and capture patterns at multiple scales while filtering out high-frequency noise. Their CNN (with dilations) effectively learned filters that span multiple non-adjacent tick intervals, smoothing out random cancellations or flickers in the LOB. This kind of design – using specialized CNN layers or preprocessing in the architecture – becomes crucial when feeding raw ticks. If one were using aggregated 1-min bars, a standard CNN might not need such elaborate dilation since noise is already reduced; a simple convolution could capture patterns like “three increasing closes in a row” without needing to filter microstructure noise.

* **Attention Span and Memory:** Transformers compute self-attention over the input sequence, which for tick data can be very long (potentially thousands of events in recent history). This raises **computational and memory considerations**: the self-attention mechanism is O(\$n^2\$) in sequence length. For a sequence of length \$n=1000\$ ticks, that’s manageable; for \$n=100,000\$ (which could be just a few hours of trading for BTC/USDT on a busy exchange), vanilla Transformers become impractical. Solutions include using *truncated or sliding windows* of tick data (the model looks at, say, the last \$N\$ ticks within a moving window), or employing Transformer variants designed for long sequences (like Transformer-XL or Longformer, which use sparse attention). Another approach is **hierarchical modeling**: e.g. first aggregate ticks into small bars or clusters internally, then apply attention on those. The choice of input representation can thus dictate whether a standard Transformer is feasible or if one needs advanced attention mechanisms. In practice, many HFT models limit input length – e.g. DeepLOB used 100 consecutive order book states as input. For a CNN-Transformer with tick input, one might similarly restrict to the last few hundred or thousand ticks, which could correspond to a short time span during high volatility or a longer span during low activity. This adaptive sequence length is another wrinkle: with irregular ticks, the window covering the last 5 minutes might contain 500 ticks during quiet times or 5000 ticks during a frenzy, making fixed-length input less straightforward. A solution is to sample by *event-based bars* inside the model – e.g. always use the last 100 volume bars as the sequence, which yields a variable actual time span but fixed-length input to the Transformer. Model designers must therefore carefully decide how to feed irregular data into an architecture that prefers fixed-size sequences.

**Handling of Irregular Sequences:** If using **raw tick sequences**, one must address their asynchronous nature. Two common strategies are: (1) *Append time deltas as a feature* – e.g. each tick record includes \$\Delta t\$ from the previous tick. The CNN or Transformer can then learn that large \$\Delta t\$ (gap) implies a different significance (often none) to connections between distant ticks. (2) *Time masking in attention* – modify the Transformer’s attention scores to incorporate a decay for distant events in wall-clock time. This can prevent the model from giving undue weight to far-apart ticks that just happen to be adjacent in the input index sequence. For CNNs, irregular intervals are less explicit: a convolution treating tick \$i\$ to \$i+4\$ equally spaced may not know that tick \$i\$ and \$i+1\$ were 1 millisecond apart while \$i+3\$ to \$i+4\$ were 10 seconds apart. Including time as a feature map helps address this.

In contrast, with **regular bars (time or constant event count)**, the notion of position in the sequence often correlates with actual time passed, simplifying model assumptions. A vanilla Transformer with positional encoding can be used directly on, say, a sequence of 1-minute bars since position = minute index. The model architecture can thus be simpler.

**Feature Embeddings and Input Layers:** The choice of input representation dictates what features the model sees initially. For time bars, typical features are OHLCV values and perhaps engineered features like returns, volatility, etc. For tick data, one might feed raw fields (price, quantity, possibly trade direction or buyer/seller initiation). Some models embed the limit order book state around each trade as well, effectively giving more context to each tick. For example, a trade tick can be represented by a feature vector: \[trade\_price, trade\_size, bid-ask spread before trade, order book imbalance before trade, etc.]. A CNN on such enriched tick vectors can detect patterns like *“large aggressive buy order when imbalance was skewed to the buy side”*, which might be a strong bullish signal. If only bars are used, such granular context is lost, so the model might rely on indirect clues (like an unusually large bar volume or range).

**Examples from Literature:** The **DeepLOB model** combined CNN and LSTM: it used three convolutional blocks to extract features from an order book snapshot sequence, then an LSTM to capture temporal dependencies. This effectively was a CNN+RNN hybrid. In later work, an attention mechanism was added to the LSTM to improve long-range pattern capture. The success of DeepLOB demonstrates that *convolutions can effectively extract microstructural features from raw LOB data,* but the addition of a recurrent/attention component was needed to model time dynamics beyond the CNN’s receptive field. Another study by Kisiel & Gorse (2023) introduced **Axial-LOB**, a fully attention-based model (no CNN) that uses axial attention to capture both local and global interactions in LOB data. They argue that CNNs, while efficient, might miss long-range dependencies unless stacked with recurrences or dilations, which increases complexity. Axial-LOB’s approach was to use attention mechanisms that can directly handle long sequences without handcrafted convolution kernels, achieving state-of-art on the same FI-2010 benchmark. The takeaway for our context: **the more raw and information-rich the input (like tick LOB data), the more the model architecture must prioritize efficient feature extraction and long-range dependency modeling**. Simpler architectures (a plain CNN or simple LSTM) may struggle with raw ticks because of the noise and sheer sequence length. This is why cutting-edge models layer components: e.g. the DTNN model has a *feature extractor module (CNN-based)* to denoise and compress the tick data into feature vectors, then a *prediction Transformer module* to attend over those features. They even add a *differential layer* that computes differences between adjacent time steps, highlighting subtle changes – a technique to emphasize new information and reduce the burden on the transformer.

In practical terms, if one uses **time bars as input**, the CNN-Transformer can be simpler (no need for dilations or special layers) and the sequence lengths are manageable, but the model might need to be deeper or have larger kernels to make up for lost detail. If using **tick-by-tick input**, one should consider architectural enhancements: e.g. *inception modules or multi-scale CNN filters* (to capture patterns at different event scales), *attention or gating mechanisms* to focus on significant events (and ignore clusters of noise), and possibly *memory modules* if the sequence is truncated (to retain some state between successive sequences). The architecture might also incorporate *order book attention* if additional data like L2 order book or trade meta-data are included.

To summarize, **different data representations mandate different CNN-Transformer design choices:** Regular, lower-frequency data allow a straightforward architecture with standard temporal kernels and positional encoding. Irregular, high-frequency tick data necessitate advanced handling – including time-aware embeddings, noise-robust convolutions, and scalable attention mechanisms – to fully exploit the data without being overwhelmed by it. When designing a CNN-Transformer for BTC/USDT tick data, one must ensure the model can handle the volume and velocity of information: this could mean limiting the input length (perhaps focusing on the most recent important events) and giving the model tools (layers) to filter out the “random walk” component of tick movements while amplifying meaningful patterns.

## 4. Computational and Practical Considerations

**Data Volume and Storage:** BTC/USDT generates massive tick-by-tick datasets. Every trade (which can be dozens per second during active periods) is a record. Storing tick data for a year can reach billions of records, easily hundreds of gigabytes of data. In contrast, storing 1-minute OHLCV bars for a year (525,600 minutes) is trivial in size. Thus, one practical consideration is *storage and data handling*. If a deep model is to be trained on tick data, one needs infrastructure to handle loading and iterating through these huge sequences. This often means using distributed storage or stream processing. Many institutional researchers note that tick data acquisition and cleaning is a large upfront cost – in fact, high-quality tick data often must be purchased (the Hudson & Thames team estimated around \$1,000 for a decent tick dataset for research). If resources are limited, one might opt to use aggregated bars which are readily available through free APIs (albeit with information loss). From a purely *pragmatic* perspective, **time bars and standard OHLCV are much easier to obtain and manage** (exchanges and data services provide them freely), whereas tick data might require connecting to exchange APIs, recording live data, or buying historical tick data. That practical constraint cannot be ignored in many projects.

**Preprocessing Time:** Converting raw ticks into alternative bars (volume, dollar, imbalance) is itself a computational task. Libraries like *mlfinlab* provide functions to do this efficiently, but for high-frequency crypto data, preprocessing can be heavy. However, this cost is typically incurred once (offline) to produce a cleaned dataset for training. The **training time** of the model, on the other hand, scales with the number of input samples and sequence length. Using 1-minute bars, you may have \~1,440 samples per day; using tick-by-tick, you could have 100,000+ samples per day. If using event bars like volume bars, the number of bars per day will depend on trading volume but might be in between (e.g. a dollar bar calibrated to \~50 bars/day for stocks might not apply to 24/7 crypto, but one could target a certain number of bars per hour). More samples means more training iterations or longer sequences per batch, both increasing training time. **Training deep models on tick data can be orders of magnitude slower** than on minute bars. For instance, a Transformer on sequences of length 1000 (tick data) will have to perform attention on a 1000x1000 matrix per layer, whereas on 60-step sequences (hourly bars for a day) it’s 60x60. Even with optimizations, expect higher GPU memory usage and longer training for tick-level models. Techniques like gradient checkpointing, mixed precision, or downsampling events (e.g. only using every 5th tick) might be needed to fit within memory.

**Online Inference and Latency:** In high-frequency trading, *latency is critical*. If your model uses tick data as input and outputs signals also at tick frequency, you must be able to compute predictions in real-time as each new tick arrives (or at least fast enough not to miss trading opportunities). A large CNN-Transformer might introduce latency that makes it impractical for actual trading decisions at sub-second scales. For example, if your model takes 100ms to update after each tick, but ticks arrive every 10ms, you’re lagging. In practice, many HFT systems using deep learning will **bucket ticks into micro-intervals** (like predictions every 100ms or every 50 trades) to give computation time for the model. Time bars naturally impose a delay (predict every 60s for 1-min bars), which is unacceptable for high-frequency strategies but fine for low-frequency strategies. So, there is a *trade-off between granularity and timeliness*: direct tick-by-tick modeling yields the most responsive signals but at highest compute cost; slightly aggregating (e.g. 1-second bars or 100-tick bars) might drastically cut computation and still retain high frequency. In a live crypto setting, one might choose a compromise like generating predictions on every **dollar bar** completion – since dollar bars complete faster in volatile times and slower in calm times, the model naturally speeds up or slows down its inference frequency based on market activity. This is advantageous as it allocates computation when it’s most needed (volatile periods) and not when it’s wasted (quiet periods).

**Scalability:** If training on tick data, parallelization and scalability become concerns. Large datasets might not fit in memory, so techniques like minibatching and shuffling need to be applied carefully (shuffling tick data must preserve chronological order in each sequence; one often does sequential training with shuffling at the sequence level). Data pipelines (e.g. reading from disk or database) can become a bottleneck when each epoch involves billions of points. Using event-based bars drastically reduces dataset size (e.g. dollar bars might yield a few thousand bars per day instead of hundreds of thousands ticks). This not only speeds up training (less steps) but can also reduce overfitting, as the model sees fewer, more meaningful data points. **Noise reduction via aggregation can act as a form of regularization**, helping the model generalize with fewer highly noisy data points.

**Non-Stationarity and Regime Changes:** Financial data are non-stationary, and crypto even more so (with regimes like bull runs, crashes, varying liquidity). With raw ticks, the model will face shifting patterns of market microstructure – e.g. the typical trade size or frequency in 2025 might be different from 2020. If using a fixed bar length (like time bars), one might easier detect or adapt to certain macro-level shifts (like volatility regimes) by looking at how bar statistics change over time. With tick data, *drifts in the data distribution* (like suddenly more high-frequency trades by bots) could affect model performance. One may need to periodically retrain or at least update scaling normalization. Some researchers apply **fractional differentiation** to the price series (Lopez de Prado’s method to make the series stationary while preserving memory) – this can be done on bar series more straightforwardly than on tick data, though in principle it’s possible on ticks too. In practice, fractional differentiation or other stationarization techniques (like applying a rolling z-score) might be applied to the input features of the model to handle non-stationarity. The *Deep High-Frequency Crypto Trend Detection* paper (Asareh Nejad *et al.*, 2024) specifically introduced a preprocessing method to ensure stationarity in 15-minute BTC bars. Their method improved performance, indicating that even at moderate frequencies, careful preprocessing is needed. At tick level, this is even more crucial – one might use techniques like **rolling window normalization** (subtract a moving average, divide by moving std) so that the model isn’t thrown off by slow changes (e.g. BTC price level doubling over a month).

**Summary of Practical Pros/Cons:**

* *Time Bars:* Easiest to handle (small data, widely available), fastest training and inference, but highest information loss and likely suboptimal for HFT where timing matters. Often serve as a baseline but may require more extensive feature engineering to get good performance.

* *Tick/Event Bars:* More data to store and process but capture more information. Volume/dollar bars can improve model training (fewer autocorrelation issues) at the cost of some complexity in data preparation. They strike a balance by compressing data in a market-aware way.

* *Raw Ticks:* Maximum fidelity, no need to make assumptions in preprocessing. But computationally expensive, and models must be carefully engineered to cope with noise and long sequences. Often requires significant computing resources (GPUs/TPUs, large memory) and possibly custom code to feed streaming data. Inference can be slow if not optimized, which might hinder deployment in ultra-low-latency environments.

One must also consider *robustness*: models trained on heavy aggregation might be more robust to slight timing differences (since they operate on a slower timescale), whereas tick-level models might be very sensitive to micro-timing (e.g. if exchange feeds drop a few ticks or if there’s a slight clock desync, the model could be thrown off). If deploying across multiple exchanges or assets, a simpler representation might be more robust. In cryptocurrencies, where exchanges can have differing microstructure, a model trained on Binance tick data might not directly transfer to Coinbase data unless the input is homogenized (bars can be a homogenizer to some extent).

Finally, **practical implementation in crypto trading** demands consideration of how to update the input in real-time. If using bars, one could update features once per bar. If using ticks, one might implement the model as a streaming process that takes each new tick, updates internal state (like CNN feature maps or Transformer memory), and outputs a prediction incrementally. This is a complex engineering task. Therefore, many practitioners start with a manageable bar interval, get something working, then gradually increase data frequency as needed and as supported by their infrastructure. The more “heavy” the data approach, the more important the engineering (parallel computing, optimized libraries, possibly C++ implementations of model inference, etc.) becomes in order to make the approach viable in a live trading setting.

## 5. Advantages and Disadvantages in the Cryptocurrency Context

Cryptocurrency markets present a unique environment for high-frequency data modeling. Key characteristics include 24/7 continuous trading, high volatility, a diverse participant base (retail and algorithmic traders globally), and evolving market microstructure. These factors influence which data representation is most effective:

* **24/7 Trading (No Market Close):** Unlike equities, crypto has no daily shutdown or opening auction. This means there are no daily gaps; patterns are more continuous, and intraday seasonality might be different (though there are still *time-of-day effects* – e.g. daily cycles when certain regions are active). Time bars in crypto thus avoid one pitfall present in equities (overnight gaps messing up OHLC calculations), but they also **miss the fact that information flow is not uniform over 24 hours**. There are still lulls (e.g. low volume often around weekends or certain hours). For example, studies have found a day-of-week effect in crypto returns despite 24/7 trading. A time-bar model might need to explicitly encode time-of-day or day-of-week to capture these periodic patterns, whereas an event-based approach inherently slows down sampling during quiet weekends and speeds up during busy weekdays, partially adjusting for these effects. One interesting observation is that the continuous nature “weakens the forecasting power of traditional models” that rely on daily closure assumptions. Models must contend with potentially trendless overnight periods or sudden news at any hour. Using finer data (tick or volume bars) can help react faster to news that breaks outside of regular hours, which is critical in crypto where, say, a tweet at 3 AM can cause an immediate price spike.

* **High Volatility:** Crypto prices can move by large percentages in a single day (or hour). This amplifies the **advantage of dollar bars**. As David Zhao notes, for assets like Bitcoin that can rise 5x in a year, using volume bars might not fully account for the price change (the same number of coins traded now represents 5x more dollar value). Dollar bars, sampled on a fixed USD traded, “help preserve the consistency and integrity of information” in a highly volatile market. In essence, dollar bars automatically adapt to price level changes – when Bitcoin’s price is high, fewer coins need to trade to hit a \$1 million bar, so bars come faster, reflecting that significant value is at risk. For a CNN-Transformer, feeding dollar bars ensures the model sees a more stationary flow of “value impact” events, which might improve learning. On the downside, dollar bars require conversion to a base currency – not an issue for BTC/USDT since USDT is already dollar-pegged, but something to note when multi-currency. Crypto’s volatility also means **raw tick data is extremely noisy**, with large outliers. Without robust normalization, a deep model might get overwhelmed by the sheer scale of jumps. However, those jumps are exactly why tick data can be valuable – a sudden 5% move within a minute will be represented as one large bar if using 1-min bars (harder to infer what happened within that minute), whereas tick data or imbalance bars might show a sequence of many buy orders eating the order book. For capturing extreme volatility events (which are common in crypto), tick-level modeling can give a clearer picture of the order flow leading to the move.

* **Market Microstructure Differences:** Crypto exchanges operate via electronic limit order books, similar to equities, but some differences exist: e.g. no centralized exchange (fragmentation across venues), varying fee structures and order types, prevalence of **maker-taker bots** and **wash trading on smaller exchanges** (which can introduce noise in tick data). A model that uses tick data might inadvertently learn exchange-specific patterns or even be misled by spurious activity (e.g. an exchange’s API prints a heartbeat trade every second of size 0.0001 BTC – which would generate ticks but no real information). Aggregated bars can act as a filter to such microstructure quirks. Another factor: crypto trades 24/7 but liquidity can fluctuate widely; during illiquid times, tick data might have odd gaps or out-of-order trades. If the model isn’t carefully built, these irregularities can cause trouble. On the bright side, crypto’s microstructure also offers some **consistent signals** – for instance, order book imbalances and large hidden orders tend to impact short-term price, similar to equities. Tick data allows extraction of these signals (like *“whale alarm” – detecting a sequence of large trades*). Whether a CNN-Transformer can capitalize on them depends on training. There is research suggesting that informed trading does occur in crypto tick data: Natashekara *et al.* (2024) found that medium-to-large trade sizes contribute disproportionately to price moves in crypto, implying that tracking those trades (only possible with tick data, not aggregated bars) could improve predictions of short-term trends.

* **24/7 and Model Retraining:** Another crypto-specific challenge is that models may need more frequent retraining or adaptation because the market regime can shift quickly (bull to bear, etc., with no downtime). If using tick data, the model might adapt faster to changing volatility regimes since it directly observes changes in trade frequency/size distribution. A time-bar model might lag because it sees aggregated effects. For example, if volatility doubles, tick frequency and volume per minute likely increase; a model noticing an increase in bar frequency (in event bars) or an increase in bar volatility can adjust its predictions (if it was trained to incorporate volatility regimes). A time-bar model would just see bigger bar ranges but might not differentiate volatility regime vs a one-off event easily.

**Advantages by Representation in Crypto:**

* *Time Bars (e.g. 1-min OHLCV):* Simpler and many crypto strategies historically use them (momentum, mean-reversion signals on 5-min bars, etc.). They benefit from decades of technical analysis indicators that are defined on such bars. They also allow easier integration of *technical indicators* and other features (RSI, MACD on 15-min bars are common even in crypto trading). For a CNN-Transformer, time bars provide a structured, lower-frequency input that might be sufficient for longer-term trend prediction (e.g. predicting the next hour’s direction). They struggle, however, to feed a high-frequency *trading* strategy (where decisions every minute might be too slow in a fast market).

* *Tick/Event Bars:* In crypto, volume and dollar bars have a clear edge in making the series more homogeneous. Research by Martinez (2019) showed that for BTC, dollar bars achieved much closer-to-normal return distributions than time bars. For a deep model, this often means easier optimization: gradients are less likely to explode if the input returns are roughly i.i.d. normal rather than highly clustered. Imbalance bars could be particularly useful in crypto’s order flow: they would generate bars more frequently during, say, sustained buying or selling pressure (common during news), potentially alerting the model of a regime change sooner than a fixed interval bar. One disadvantage is complexity and parameters – one has to choose the thresholds or initial estimates for imbalance bars, and these might need tuning if the market’s typical trade sizes change.

* *Raw Tick Sequence:* The main advantage is **maximum responsiveness**. A model watching tick-by-tick can, in theory, react to patterns like “a flurry of aggressive buys lifting the ask” within milliseconds, whereas even a 1-min bar model would only see the outcome after the fact. In a market where arbitrageurs and HFT firms are active, being early by even a few seconds can be significant. For a CNN-Transformer, having tick inputs means it could learn patterns like those seen in order book dynamics (like the *shape* of a price impact). Additionally, crypto being relatively new and less efficient than say FX, there may be more alpha in the microstructure (inefficiencies that last a few seconds) that a tick model can capture but a bar model would miss. The disadvantage is, as discussed, the cost of processing and the risk of learning noise. Crypto ticks can also have artifacts – e.g. if an exchange has batching, one tick might represent a bundle of trades reported together. The model might misinterpret that if not careful (though volume info can help distinguish it).

* *Hybrid Approaches in Crypto:* A hybrid (discussed more in Section 6) can be very powerful in crypto. For example, one could use a Transformer to incorporate *on-chain data or funding rate data on a lower frequency* alongside a CNN on tick data for short-term moves. Crypto markets are influenced by a mix of high-frequency order flow and slower fundamental flows (like large transfers, funding rate arbitrage). A hybrid model can ingest both. This is an advantage unique to crypto: the availability of alternative data (like mempool transaction rates, miner statistics) that have no analogy in traditional HFT. These typically come as slower signals (e.g. one value per minute or hour). Combining that with tick-by-tick trading data can improve context.

**In summary**, in the crypto context **event-driven representations (especially dollar bars) often outperform time bars** because they naturally adjust to the market’s volatility and uneven activity. They ensure that each input to the model carries a comparable “amount” of market information, which is crucial when activity can vary 10x between New Year’s Day and a frenzy during a Bitcoin rally. However, the **ultimate performance depends on the model’s objective**. If we are building a *market-making model* or very short-term predictor, raw tick data is almost mandatory to compete with other HFTs (since decisions need to be made at the speed of incoming trades). If instead the model is for slightly lower frequency trading (say predicting the next 5-minute return direction), one could argue that the noise in ticks is unnecessary and aggregating to a few-second bars might suffice and be more robust.

One must also consider **latency to exchanges**: crypto exchanges often have API rate limits and network latency around tens of milliseconds. A model operating on tick data must be colocated or extremely optimized to act faster than competitors. If that infrastructure is not available, using slightly coarser data might not actually degrade real-world results, because you couldn’t act on the super-fast signals anyway. Many crypto quant funds thus operate on sub-minute bars (like 5-second or 1-second bars) which still capture much intraday variability but are easier to manage than pure ticks. This could be seen as a practical sweet spot.

## 6. Hybrid Models and Feature Augmentation

Given the strengths and weaknesses of each approach, **hybrid models** attempt to combine multiple representations or feature types to maximize predictive power. In the context of feeding data to a CNN-Transformer, “hybrid” can refer to several design aspects:

* **Multiple Time-Scale Inputs:** The model can ingest both high-frequency and lower-frequency data streams. For example, a hybrid CNN-Transformer might have one branch that processes tick-by-tick data (perhaps compressed through a CNN) and another branch that processes 1-minute bar data or technical indicators over a longer window. The outputs of both branches can then be fused (concatenated) and passed to a final prediction layer or an attention mechanism that weighs them. This way, the model doesn’t have to choose one representation; it gets the fine detail from ticks and the broader context from bars. In practice, this could capture situations where tick patterns indicate a short-term order flow imbalance, but the higher-level trend (from the bars) might either reinforce or counteract that signal. If both agree (e.g. bullish order flow and an ongoing uptrend), confidence is higher; if they conflict (bullish blip against a bearish trend), the model can modulate its prediction accordingly. Such multi-scale architectures have been explored in time-series deep learning. For instance, *Temporal Fusion Transformer* (Lim et al. 2020) includes both high-frequency time series and low-frequency static covariates in its attention. Similarly, Kim et al.’s attention CNN-LSTM for crypto (2023) effectively integrated multiple frequency data by design.

* **Tick Data Enriched with Bar Features:** Another hybrid approach is to **augment tick sequences with features computed on the fly from those ticks**. Instead of providing only raw price/volume per tick, we could supply the model with rolling statistics. For example, for each tick, include the 1-second return or the 10-trade moving average of trade size as additional features. Véber (2020) suggests that we can treat bars as *tensors* of features, potentially multi-dimensional. One can derive various features from tick data (like volatility estimates, order imbalance, etc.) at different horizons and stack them. In his example, he imagines a 2×N input where one row is features from the bid side, one from the ask side, which a 2D convolution can exploit similarly to how image CNNs use color channels. Extending this idea, we might compute features from different *timeframes*. For instance, at any given tick (which we’ll call the “anchor” point), we can compute the OHLC over the last 5 seconds, 30 seconds, 5 minutes, etc., and feed those as additional channels/features to the model. This gives the model knowledge of the short-term bar structure around that tick, essentially merging bar info into tick modeling. Véber notes that *using features or statistics of different timeframes ending at the anchor tick* is a powerful way to enrich the input. A CNN-Transformer can then, for example, detect a pattern like “tick price is surging above the 1-min VWAP and 1-hour trend is up” – a multi-scale condition that might be very predictive.

* **Parallel Models for Different Data:** A hybrid model could also be implemented as two separate models whose signals are combined (ensemble or meta-model). For example, a CNN-Transformer A trained on tick data for microstructure signals, and model B (maybe a simpler LSTM or even a fundamental model) on lower-frequency data (like hourly sentiment or funding rates). One could then input both predictions into a meta-model or simply take a weighted average. While not a single integrated model, this approach can be effective and easier to develop modularly.

* **Feature Fusion with Attention:** Within a single CNN-Transformer, one can use the attention mechanism itself to perform fusion. For instance, you could concatenate a sequence of tick embeddings with a sequence of bar embeddings (imagine one long sequence that interweaves, say, 5-second bars and tick events). The Transformer’s self-attention could learn to attend across these different “kinds” of inputs to find relevant correlations (somewhat analogous to how Transformers in NLP handle sequences of words and additional info like part-of-speech tags). Alternatively, use *cross-attention*: have the Transformer attend to tick sequence encoded by CNN as the query, and the key/value come from a context vector of higher timeframe indicators. This way, when making a prediction from ticks, the model can “peek” at the larger trend context.

**Real-World Examples of Hybrid Approaches:** In quantitative trading, multi-scale approaches are common. For example, some high-frequency trading strategies incorporate *trend filters from higher timeframes* to decide when to engage. A deep learning analog is to provide those trend signals to the model. Xu *et al.* (2019) presented a hybrid CNN-BiLSTM that fused news sentiment with technical features, which is another form of feature augmentation (mixing fundamentally different data sources). In crypto, you might feed on-chain metrics (like active addresses, hash rate trends) alongside tick data. While a CNN-Transformer might not natively handle such disparate frequencies, one can preprocess the slow features to align with the tick timeline (e.g. repeat the daily on-chain value for all ticks within that day as a feature).

One particularly relevant idea is **meta-labeling and ensemble signals**. Lopez de Prado’s concept of meta-labeling could be seen as a hybrid: use a primary model (perhaps based on technical bars) to generate candidate signals, and then use a secondary model (perhaps with tick data) to decide which signals to follow. For instance, a primary model flags a potential breakout based on 15-minute bars; a secondary CNN on tick data then verifies if order flow supports the breakout (lots of aggressive buys) – if yes, it confirms the signal, if not, it filters it out. In a deep learning context, this could be implemented as one network feeding its output as input feature into another.

**Design Considerations for Hybrid Models:** While powerful, hybrids can become **complex and risk overfitting** if not managed. There’s a danger of adding too many inputs and the model finding spurious correlations. It’s important to use regularization and cross-validation on each component. Also, training can be heavier – you might need to train two sub-networks (one on ticks, one on bars) either sequentially or jointly, which is more complicated than a single-network training. However, the benefits often outweigh these costs. Hybrid models capitalize on the complementary strengths of each data type: tick data provides *precision and reaction speed*, bar data provides *robustness and contextual awareness*.

In the context of BTC/USDT with a CNN-Transformer, a feasible hybrid setup could be:

* A **CNN module** that takes the last, say, 100 ticks (with features like price change, volume, maybe order book imbalance) and produces a feature embedding summarizing the short-term microstructure state.
* A **Transformer module** that takes a longer sequence of, say, 1-minute bars over the last day, capturing the macro trend, volatility regime, etc.
* A **fusion layer** that combines these (e.g. concatenating the CNN’s output with the Transformer’s output or passing them through another attention layer).
* The combined representation then goes to a prediction head (which could be another small neural net) to output the next move prediction or trading signal.

This design allows the CNN to be specialized for micro-scale pattern recognition (like detecting a burst of buying or a spoofing event in the order book), while the Transformer focuses on time correlations at the bar level (like detecting that the model should trust buy signals more during an uptrend). During training, such a model learns to weigh features from both scales. For example, it might learn that during sideways markets (detected by Transformer from bar sequence), tick-level signals are mostly noise and shouldn’t trigger a trade, whereas during strongly trending markets, tick patterns like pull-back-and-bounce are reliable entry opportunities.

Empirical evidence in equity markets supports multi-scale modeling. For instance, one paper combined *wavelet-based multi-resolution analysis* with deep learning to capture both short and long-term patterns, yielding improved forecast accuracy (the wavelet essentially split the time series into multiple frequency bands, which were then learned by different sub-networks).

**Hybrid Feature Augmentation in Practice:** A concrete augmentation for crypto could be adding **order flow features** to bar data. For each bar (say a 1-min bar), you could attach features like “number of buy ticks vs sell ticks in that bar” or “volume imbalance” or “average trade size” during that bar. These are derived from tick data but summarized at the bar level, giving the model some microstructure insight without feeding every tick. This approach has been used in some high-frequency signal research – essentially enriching OHLCV with microstructure indicators. If one doesn’t want to go full tick-by-tick, this intermediate step can capture a lot of the value from tick data. For instance, a study by Chen *et al.* (2021) found that adding order imbalance features to a minutes-bar LSTM improved prediction of short-term price moves in futures markets. We can do similar for BTC/USDT.

Finally, note that hybrid approaches might also refer to combining model types (like a deep model with a simple model). For example, using a statistical arbitrage model to find opportunities and a deep model to time entry. But in terms of data representation, the key point is: **combining multiple representations generally yields richer models**. The CNN-Transformer architecture is flexible enough to accept multiple inputs, and attention mechanisms are naturally suited to fusing information from different sources by weighting their importance.

In conclusion, a hybrid approach – whether multi-scale, multi-source, or feature-augmented – often provides the best performance in complex domains like cryptocurrency trading. It leverages the **noise-reduction and context of aggregated data** alongside the **granularity and immediacy of tick data**. As a best practice, one should not feel forced to pick exactly one input representation; instead, consider a design that allows the model to draw from several representations. This can mitigate the weaknesses of each: for instance, if tick data is too noisy alone, the model can be guided by bar data; if bar data is too slow, the tick input can alert the model to quick changes. The result is typically a more robust and performant trading signal.

<br>

**Comparative Summary:** The table below synthesizes the advantages and disadvantages of the various data representation approaches discussed, especially as they pertain to a CNN-Transformer model in high-frequency crypto trading:

| **Data Representation**                                  | **Sampling Basis**                | **Pros** (Info & Modeling)                                                                                                                                                                                                                                                                      | **Cons** (Info & Modeling)                                                                                                                                                                                                                                                                                      | **Crypto-Specific Notes**                                                                                                                                                                                                                              |
| -------------------------------------------------------- | --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Time Bars (e.g. 1-min)**                               | Fixed clock time                  | – Simple, widely available, low data volume<br>– Smooths noise, easier model convergence<br>– Captures broad trends and technical patterns                                                                                                                                                      | – Disguises varying market activity, loses microstructure detail<br>– Poor statistical properties (serial correlation, non-normal returns)<br>– Slow to react to sudden changes (latency up to bar interval)                                                                                                    | Common baseline in crypto; may miss fast moves in 24/7 market, needs additional features to perform well in high volatility periods.                                                                                                                   |
| **Tick Bars** (every *N* trades)                         | Fixed number of trades            | – Aligns sampling with market activity, more info per bar<br>– Reduces heteroskedasticity vs time bars<br>– Each bar has identical count of trades, simplifying analysis of trade sequences                                                                                                     | – One tick != one trade size (trades vary); large trades can distort bars<br>– Still somewhat arbitrary (choice of *N*)<br>– Outliers (exchange batch trades) can skew tick bars                                                                                                                                | In crypto, trade sizes vary widely (from micro-lots to whale orders), so tick bars can be inconsistent. Often used with caution or combined with volume info.                                                                                          |
| **Volume Bars** (every *X* units traded)                 | Fixed volume traded               | – Normalizes bars by trading volume, improving return normality<br>– Captures price-volume relationship better<br>– Mitigates issues of tick size variance (each bar \~equal market participation)                                                                                              | – Needs appropriate volume threshold (which may change with volatility)<br>– Still not price-adjusted; if price changes drastically, volume bar frequency shifts (can be addressed by dollar bars)<br>– More complex to implement in real-time (cumulative volume tracking)                                     | For BTC/USDT, volume bars are effective during stable periods. In bull runs, as price rises, a fixed coin-volume bar will result in increasing dollar value per bar, potentially overweighting later data – dollar bars solve this.                    |
| **Dollar Bars** (every *X* USD traded)                   | Fixed value traded                | – Adjusts for price fluctuations; robust under changing price levels<br>– Best statistical properties (returns closest to normal, lowest autocorrelation)<br>– Preserves info integrity in highly volatile assets                                                                               | – Requires conversion to dollar volume; for direct USDt pairs this is natural<br>– Threshold *X* may need periodic recalibration (e.g. to target \~50 bars/day as suggested by de Prado)<br>– Loses the direct interpretation in terms of number of trades or coins (purely value-based)                        | Particularly suited for crypto (which has large price swings). E.g., a \$10M bar will consistently represent significant market interest regardless of BTC price. Many crypto quant firms favor dollar bars for strategy development due to stability. |
| **Imbalance Bars** (information-driven)                  | Triggered by order flow imbalance | – Samples *more frequently when informed trading likely occurs*, enabling early trend change detection<br>– Dynamic and adaptive: essentially *event-triggered bars* based on market microstructure signals<br>– Often yields more i.i.d. returns and can reduce structural breaks in series    | – Complex to implement and tune (need to estimate expected imbalance, threshold)<br>– May produce irregular and sparse sampling in calm markets (which can be okay, but model must handle variable intervals)<br>– Less common in practice; tooling (like mlfinlab) exists but not as widely used as fixed bars | Could be very powerful in crypto where order flow can shift quickly (e.g., detecting when buys suddenly overwhelm sells). But the method’s parameters might need re-fitting as crypto markets evolve.                                                  |
| **Raw Tick-by-Tick** (no aggregation)                    | Each trade event (irregular time) | – Maximal information, no aggregation loss – model can learn any pattern in order flow<br>– Highest temporal resolution; fastest reaction possible<br>– Avoids picking a sampling threshold (no bias from bar definitions)                                                                      | – Very high data frequency and volume → heavy computational load and storage demand<br>– Noisy: contains microstructure “micro-noise” (bid-ask bounce, fleeting orders) that model must filter<br>– Irregular timing complicates model input (requires time feature encoding or resampling)                     | For HFT algo development in crypto, tick data is essential. However, pure tick models require significant engineering (low-latency pipeline, robust model to noise). Often used in conjunction with order book data.                                   |
| **Hybrid Approach** (e.g. multi-scale or feature fusion) | Combination of above              | – Leverages strengths of multiple representations (e.g. trends from bars + patterns from ticks)<br>– Can improve robustness: if one input is noisy, other can guide the model<br>– Offers richer feature space (e.g. tick-based order flow indicators alongside bar-based technical indicators) | – More complex architecture (multiple inputs, higher risk of overfitting if not enough data)<br>– Longer development and tuning time (must ensure different data streams align in model)<br>– Higher computational requirements if processing both ticks and bars concurrently                                  | Highly promising for crypto: e.g., combine on-chain or sentiment features (daily) with tick data (second-by-second). Many cutting-edge crypto models use some form of hybrid, though it requires expertise to implement.                               |

As the table highlights, each approach has clear pros and cons. In practice, a researcher often starts with time bars or volume/dollar bars to get an initial model working, then progressively incorporates more granular data (ticks) or additional features once the baseline is set. The optimal solution for a CNN-Transformer will depend on the specific trading strategy’s horizon and the computational resources available. For true high-frequency trading (sub-second decisions) in BTC/USDT, one will eventually need to incorporate raw tick data or very granular event bars, and design the model to handle them. If the goal is more to predict short-term trends on the order of minutes, volume or dollar bars paired with a well-tuned Transformer might suffice and will be much easier to manage.

## Conclusion

Feeding BTC/USDT tick-by-tick data into a CNN-Transformer model requires a careful evaluation of how to represent that data in a way that maximizes signal extraction while remaining computationally feasible. **Traditional time bars** (like 1-minute OHLCV) offer simplicity and noise reduction but suffer significant information loss and poor statistical properties for high-frequency modeling. **Event-driven bars** – tick bars, volume bars, dollar bars, and the more adaptive imbalance bars – provide a more information-rich and stationarity-friendly input, as evidenced by their closer-to-normal return distributions and lower autocorrelations. These representations help a deep learning model by delivering more homogeneous data chunks that align with actual market activity, thereby improving learning efficiency and potentially model accuracy.

Empirical studies and cutting-edge industry practice indicate that **model performance can be materially improved by using higher-frequency inputs**, especially when advanced architectures are employed to tame the noise. Deep learning models, from CNNs and LSTMs to Transformers, have shown superior predictive power when fed rich limit order book and tick data, as long as they incorporate design elements (like convolutional denoising, attention mechanisms, and differential features) to handle the chaotic nature of ticks. In cryptocurrency markets, where **volatility is high and 24/7 trading blurs traditional boundaries**, these high-frequency approaches allow models to catch the rapid swings and regime shifts that coarser data might miss. Dollar bars have been highlighted as particularly useful for volatile assets like BTC, preserving informational integrity across large price moves.

A **CNN-Transformer architecture** must be adapted to the data representation: for aggregated bars, it can be relatively straightforward, whereas for raw tick sequences it should include mechanisms for irregular time steps and noise filtration. Computationally, using tick data significantly increases the burden – more data to store, longer sequences to process, and potentially higher latency in live trading. However, through techniques like minibatch training, windowed inference, and by leveraging modern hardware, these challenges can be met. The practitioner must weigh the benefit of extra information against the cost of complexity. Often, a **hybrid approach yields the best trade-off**: using event-based bars to structure the data and adding tick-level features or parallel tick-based modules to inject high-frequency insights. Such hybrid models can exploit multi-scale information – a particularly potent strategy in crypto, where long-term sentiment and immediate order flow can both drive price.

In closing, there is no one-size-fits-all answer – the *optimal method* depends on the specific objectives (e.g. market making vs swing trading), the available infrastructure, and the characteristics of the data during the period of interest. A reasonable progression for research is: start with **dollar or volume bars** as input to a CNN or CNN-LSTM to establish a baseline; then incorporate a **Transformer** for improved sequence learning and compare representations (time vs event bars). Next, experiment with adding **tick-level data** – either directly or as derived features – and observe the improvement. Ensure to validate results on multiple market regimes, as the more complex models might shine particularly during high volatility or atypical conditions where their additional information helps navigate chaos. By iterating in this way, one can systematically approach the optimal input representation. The literature and our analysis strongly suggest that moving away from simplistic time bars towards more **information-aligned bars or tick sequences will provide a CNN-Transformer the richest learning ground**, ultimately leading to better performance in the fast-paced world of crypto high-frequency trading.

### References

1. **Lopez de Prado, Marcos**. *Advances in Financial Machine Learning*. Wiley, 2018. – (Introduces the concept of various bar types – time, tick, volume, dollar, imbalance – and argues for event-driven sampling to fix the inefficiencies of time bars. Provides theoretical and empirical support for volume/dollar bars yielding i.i.d. returns and better statistical properties.)
2. **Easley, D., López de Prado, M., & O’Hara, M.** (2012). “The Volume Clock: Insights into the High Frequency Paradigm.” *Journal of Portfolio Management, 39*(1), 19–29. – (Describes how measuring time by volume (“volume clock”) can detect toxic order flow and improve prediction. Forms the basis for volume bars and the VPIN metric. Empirical results show volume-sampled returns more stable, inspiring later use of dollar bars.)
3. **Zhao, David** (2020). “Stop using Time Bars for Financial Machine Learning.” *Medium (Towards Data Science)*. – (Practitioner-friendly summary of Lopez de Prado’s Chapter 2. Explains why time bars introduce noise and have poor normality, and walks through constructing tick, volume, and dollar bars for crypto data. Emphasizes that input data structure is critical for ML.)
4. **Martínez, Gerard** (2019). “Advanced Candlesticks for Machine Learning (Part I, II, III).” *Medium (TDS Archive)*. – (Series of articles on tick bars, volume/dollar bars, and imbalance bars. Provides rigorous yet accessible explanation of each bar type’s construction and their statistical evaluation. Shows, for example, Jarque-Bera normality test results where dollar bars fare best and discusses how imbalance bars anticipate trend changes.)
5. **Hudson & Thames (J. Joubert et al.)** (2019). “Does Meta-Labeling Add to Signal Efficacy? – Appendix on Sampling.” *Hudson & Thames White Paper*. – (Open-source quant research that, among other things, compares time vs tick/volume/dollar bars on futures data. Demonstrates with figures that dollar bars have lowest autocorrelation and closest-to-normal returns. Concludes that event-based sampling improves machine learning model performance.)
6. **Zhang, Zihao, Zohren, Stefan, & Roberts, Stephen** (2019). “DeepLOB: Deep Convolutional Neural Networks for Limit Order Books.” *Quantitative Finance, 19*(4), 537–556. – (Pioneering work applying CNN (with an LSTM) to high-frequency limit order book data. Achieved state-of-art on a public LOB dataset. Illustrates how CNNs can extract spatial and short-term features from level II tick data, and LSTMs capture longer sequences. Validates the efficacy of using raw order/tick data in deep models.)
7. **Lai, Shijie, et al.** (2023). “Predicting High-Frequency Stock Movement with Differential Transformer Neural Network.” *Electronics, 12*(13), 2943. – (Develops a CNN+Transformer hybrid for high-frequency data, introducing a temporal CNN (TCN) to denoise and a differential layer to highlight changes, feeding into a Transformer. Outperforms existing models on FI-2010 LOB data. Demonstrates architectural strategies needed for tick data – e.g. combining conv layers for feature extraction with attention for sequence modeling.)
8. **Kisiel, Damian & Gorse, Denise** (2023). “Axial-LOB: High-Frequency Trading with Axial Attention.” *Proceedings of NeurIPS 2023 (arXiv:2212.01807)*. – (Proposes a fully attentional architecture for LOB data, replacing CNNs with axial attention to capture global dependencies without exploding parameter count. Achieves new SOTA on benchmark. Highlights the limitations of pure CNN (locality) and how adding attention (or in this case using only attention) addresses long-range patterns in tick data.)
9. **Omole, O. & Enke, D.** (2024). “Deep learning for Bitcoin price direction prediction: models and trading strategies empirically compared.” *Financial Innovation, 10:117*. – (Compares CNN-LSTM, LSTNet, TCN, etc., on Bitcoin data using on-chain and price features. Found CNN-LSTM with proper feature selection gave \~82% accuracy. While focused on daily/on-chain data, it demonstrates the value of combining CNNs (for local patterns) and LSTMs (for longer memory) in crypto context, supporting hybrid modeling.)
10. **Kim, Woojin, et al.** (2023). “Attention-based CNN–LSTM for high-frequency multiple cryptocurrency trend prediction.” *Expert Systems with Applications, 237*, 121520. – (Introduces a labeling method and an attention CNN-LSTM model that fuses multiple frequency data for crypto prediction. Shows improved performance by exploiting both high-frequency tick data and lower-frequency features together. Validates the multi-scale approach in a crypto setting.)
11. **Véber, István** (2020). “Financial bars at the age of deep learning.” *Analytics Vidhya (Medium)*. – (Explores how to construct bar “tensors” with multiple features and timeframes for deep learning inputs. Provides insights on why to use bars at all (computation and model sequence length issues), and suggests creative ways to structure multi-timeframe data for CNNs. A practical guide bridging data resampling techniques with model layer considerations.)
12. **Natashekara, Karthik, et al.** (2024). “Informed trading and cryptocurrencies: New evidence using tick-by-tick data.” *Finance Research Letters, 61*, 104909. – (Analyzes nearly 80 million crypto trade ticks to examine how trade size buckets (small/medium/large) contribute to price discovery. Finds that medium and large trades have disproportionate impact, implying that monitoring tick data for such trades is informative. Supports the idea that tick data contains valuable signals not visible in aggregate.)
